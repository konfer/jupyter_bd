{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文自然语言处理分析\n",
    "by 寒小阳(hanxiaoyang.ml@gmail.com)\n",
    "\n",
    "和拉丁语系不同，亚洲语言是不用空格分开每个有意义的词的。而当我们进行自然语言处理的时候，大部分情况下，词汇是我们对句子和文章理解的基础，因此需要一个工具去把完整的文本中分解成粒度更细的词。\n",
    "\n",
    "### 1、关键词提取\n",
    "#### 基于 TF-IDF 算法的关键词抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jieba.analyse\n",
    "\n",
    "- jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())\n",
    "\n",
    " - sentence 为待提取的文本\n",
    "\n",
    " - topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20\n",
    "\n",
    " - withWeight 为是否一并返回关键词权重值，默认值为 False\n",
    "\n",
    " - allowPOS 仅包括指定词性的词，默认值为空，即不筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:09:42.560315Z",
     "start_time": "2022-09-02T04:09:28.917886Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.666 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户  2016  互联网  手机  平台  人工智能  百度  2017  智能  技术  数据  360  服务  直播  产品  企业  安全  视频  移动  应用  网络  行业  游戏  机器人  电商  内容  中国  领域  通过  发展\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/usr/local/codeData/jupyterData/jupyter-bd/textCategorization/data/technology_news.csv\", encoding='utf-8')\n",
    "df = df.dropna()\n",
    "lines=df.content.values.tolist()\n",
    "content = \"\".join(lines)\n",
    "print (\"  \".join(analyse.extract_tags(content, topK=30, withWeight=False, allowPOS=())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:09:49.121516Z",
     "start_time": "2022-09-02T04:09:42.562655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "航母  训练  海军  中国  官兵  部队  编队  作战  10  任务  美国  导弹  能力  20  2016  军事  无人机  装备  进行  记者  我们  军队  安全  保障  12  战略  军人  日本  南海  战机\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/usr/local/codeData/jupyterData/jupyter-bd/textCategorization/data/military_news.csv\", encoding='utf-8')\n",
    "df = df.dropna()\n",
    "lines=df.content.values.tolist()\n",
    "content = \"\".join(lines)\n",
    "print (\"  \".join(analyse.extract_tags(content, topK=30, withWeight=False, allowPOS=())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于 TextRank 算法的关键词抽取\n",
    "- jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。\n",
    "- jieba.analyse.TextRank() 新建自定义 TextRank 实例\n",
    "\n",
    "算法论文： TextRank: Bringing Order into Texts\n",
    "\n",
    "基本思想:\n",
    "\n",
    "- 将待抽取关键词的文本进行分词\n",
    "- 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图\n",
    "- 计算图中节点的PageRank，注意是无向带权图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:11:26.215377Z",
     "start_time": "2022-09-02T04:09:49.123575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国  海军  训练  美国  部队  进行  官兵  航母  作战  任务  能力  军事  发展  工作  国家  问题  建设  导弹  编队  记者\n",
      "---------------------我是分割线----------------\n",
      "中国  海军  美国  部队  官兵  航母  军事  国家  任务  能力  导弹  技术  问题  日本  军队  编队  装备  系统  记者  战略\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/usr/local/codeData/jupyterData/jupyter-bd/textCategorization/data/military_news.csv\", encoding='utf-8')\n",
    "df = df.dropna()\n",
    "lines=df.content.values.tolist()\n",
    "content = \"\".join(lines)\n",
    "\n",
    "print (\"  \".join(analyse.textrank(content, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))))\n",
    "print (\"---------------------我是分割线----------------\")\n",
    "print (\"  \".join(analyse.textrank(content, topK=20, withWeight=False, allowPOS=('ns', 'n'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、LDA主题模型\n",
    "咱们来用LDA主题模型建模，看看这些新闻主要在说哪些topic。\n",
    "\n",
    "首先我们要把文本内容处理成固定的格式，一个包含句子的list，list中每个元素是分词后的词list。类似下面这个样子。\n",
    "\n",
    "[[第，一，条，新闻，在，这里],[第，二，条，新闻，在，这里],[这，是，在，做， 什么],...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:11:26.670817Z",
     "start_time": "2022-09-02T04:11:26.218097Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 载入停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:11:26.682837Z",
     "start_time": "2022-09-02T04:11:26.673773Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords=pd.read_csv(\"/usr/local/codeData/jupyterData/jupyter-bd/textCategorization/data/stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "stopwords=stopwords['stopword'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:11:26.698310Z",
     "start_time": "2022-09-02T04:11:26.685559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['!', '\"', '#', ..., '07', '08', '09'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 转换成合适的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:12:27.302316Z",
     "start_time": "2022-09-02T04:11:26.700515Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/usr/local/codeData/jupyterData/jupyter-bd/textCategorization/data/technology_news.csv\", encoding='utf-8')\n",
    "df = df.dropna()\n",
    "lines=df.content.values.tolist()\n",
    "\n",
    "sentences=[]\n",
    "for line in lines:\n",
    "    try:\n",
    "        segs=jieba.lcut(line)\n",
    "        segs = list(filter(lambda x:len(x)>1 and x not in stopwords , segs))\n",
    "#         segs = filter(lambda x:len(x)>1, segs)\n",
    "#         segs = filter(lambda x:x not in stopwords, segs)\n",
    "        sentences.append(segs)\n",
    "    except Exception:\n",
    "        print (line)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:12:27.307588Z",
     "start_time": "2022-09-02T04:12:27.304269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本次\n",
      "商汤\n",
      "带来\n",
      "黄仁勋\n",
      "展示\n",
      "遥相呼应\n",
      "SenseFace\n",
      "人脸\n",
      "布控\n",
      "系统\n",
      "千万级\n",
      "人员\n",
      "库中\n",
      "300ms\n",
      "识别\n",
      "瞬间\n",
      "锁定目标\n",
      "功耗\n",
      "十几\n",
      "当属\n",
      "人脸\n",
      "布控\n",
      "一大\n",
      "科技\n"
     ]
    }
   ],
   "source": [
    "for word in sentences[5]:\n",
    "    print (word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:12:28.890322Z",
     "start_time": "2022-09-02T04:12:27.309287Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(sentences)\n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:12:43.431210Z",
     "start_time": "2022-09-02T04:12:28.893479Z"
    }
   },
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们查一下第3号分类，其中最常出现的单词是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:12:43.441102Z",
     "start_time": "2022-09-02T04:12:43.433694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.038*\"中国\" + 0.031*\"科技\" + 0.018*\"互联网\" + 0.015*\"创新\" + 0.014*\"产业\"\n"
     ]
    }
   ],
   "source": [
    "print (lda.print_topic(3, topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把所有的主题打印出来看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:12:43.461944Z",
     "start_time": "2022-09-02T04:12:43.443897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034*\"直播\" + 0.030*\"公司\" + 0.018*\"游戏\" + 0.016*\"汽车\" + 0.016*\"业务\" + 0.014*\"亿元\" + 0.013*\"投资\" + 0.012*\"增长\"\n",
      "0.050*\"内容\" + 0.033*\"视频\" + 0.020*\"平台\" + 0.012*\"生态\" + 0.011*\"增长\" + 0.010*\"创业\" + 0.009*\"OTT\" + 0.008*\"媒体\"\n",
      "0.028*\"孩子\" + 0.027*\"共享\" + 0.022*\"家长\" + 0.019*\"单车\" + 0.015*\"无人机\" + 0.013*\"手机\" + 0.012*\"出行\" + 0.011*\"小时\"\n",
      "0.038*\"中国\" + 0.031*\"科技\" + 0.018*\"互联网\" + 0.015*\"创新\" + 0.014*\"产业\" + 0.010*\"2017\" + 0.010*\"企业\" + 0.009*\"发展\"\n",
      "0.030*\"技术\" + 0.025*\"发展\" + 0.021*\"人工智能\" + 0.015*\"企业\" + 0.015*\"领域\" + 0.014*\"互联网\" + 0.013*\"行业\" + 0.013*\"数据\"\n",
      "0.019*\"网秦\" + 0.016*\"知识\" + 0.016*\"付费\" + 0.013*\"费用\" + 0.012*\"陌陌\" + 0.012*\"视频\" + 0.011*\"社交\" + 0.009*\"平台\"\n",
      "0.031*\"旅游\" + 0.030*\"酒店\" + 0.022*\"携程\" + 0.012*\"服务\" + 0.012*\"外卖\" + 0.011*\"旅行\" + 0.010*\"AMD\" + 0.010*\"网络攻击\"\n",
      "0.040*\"百度\" + 0.035*\"用户\" + 0.016*\"手机\" + 0.015*\"地图\" + 0.013*\"搜索\" + 0.013*\"功能\" + 0.011*\"360\" + 0.009*\"APP\"\n",
      "0.027*\"信息\" + 0.024*\"用户\" + 0.018*\"网络\" + 0.017*\"支付\" + 0.017*\"诈骗\" + 0.013*\"软件\" + 0.012*\"手机\" + 0.009*\"保护\"\n",
      "0.032*\"学习\" + 0.021*\"驾驶\" + 0.020*\"机器\" + 0.018*\"算法\" + 0.018*\"百度\" + 0.016*\"深度\" + 0.016*\"人工智能\" + 0.016*\"自动\"\n",
      "0.064*\"智能\" + 0.037*\"机器人\" + 0.019*\"服务\" + 0.014*\"工作\" + 0.013*\"办公\" + 0.010*\"宽带\" + 0.009*\"场景\" + 0.009*\"硬件\"\n",
      "0.034*\"用户\" + 0.028*\"市场\" + 0.022*\"手机\" + 0.020*\"游戏\" + 0.018*\"产品\" + 0.015*\"品牌\" + 0.011*\"中国\" + 0.011*\"营销\"\n",
      "0.058*\"数据\" + 0.016*\"企业\" + 0.016*\"智能\" + 0.016*\"服务\" + 0.014*\"技术\" + 0.011*\"系统\" + 0.011*\"提供\" + 0.010*\"电视\"\n",
      "0.021*\"北京\" + 0.019*\"城市\" + 0.011*\"上海\" + 0.011*\"视频\" + 0.010*\"网络\" + 0.009*\"比特\" + 0.009*\"发布会\" + 0.009*\"地区\"\n",
      "0.062*\"病毒\" + 0.024*\"攻击\" + 0.021*\"网络\" + 0.020*\"永恒\" + 0.015*\"事件\" + 0.013*\"相关\" + 0.011*\"利用\" + 0.010*\"黑客\"\n",
      "0.021*\"感染\" + 0.014*\"微信\" + 0.013*\"平台\" + 0.011*\"用户\" + 0.010*\"入驻\" + 0.009*\"资费\" + 0.008*\"表情\" + 0.007*\"一点\"\n",
      "0.035*\"360\" + 0.030*\"网络安全\" + 0.029*\"漏洞\" + 0.019*\"威胁\" + 0.018*\"攻击\" + 0.017*\"乐视\" + 0.014*\"政务\" + 0.013*\"网站\"\n",
      "0.035*\"VR\" + 0.021*\"小米\" + 0.016*\"体验\" + 0.015*\"产品\" + 0.015*\"摄像头\" + 0.013*\"猎豹\" + 0.012*\"手机\" + 0.010*\"公益\"\n",
      "0.024*\"经济\" + 0.015*\"体系\" + 0.014*\"论坛\" + 0.011*\"交易\" + 0.011*\"国家\" + 0.011*\"一路\" + 0.010*\"一带\" + 0.010*\"数据\"\n",
      "0.033*\"电商\" + 0.017*\"消费者\" + 0.015*\"商品\" + 0.014*\"国美\" + 0.013*\"跨境\" + 0.011*\"知识产权\" + 0.011*\"安防\" + 0.010*\"商家\"\n"
     ]
    }
   ],
   "source": [
    "for topic in lda.print_topics(num_topics=20, num_words=8):\n",
    "    print (topic[1])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
