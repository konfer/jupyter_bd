{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用机器学习方法完成中文文本分类\n",
    "by 寒小阳(hanxiaoyang.ml@gmail.com)\n",
    "\n",
    "### 朴素贝叶斯\n",
    "我们试试用朴素贝叶斯完成一个中文文本分类器，一般在数据量足够，数据丰富度够的情况下，用朴素贝叶斯完成这个任务，准确度还是很不错的。\n",
    "\n",
    "机器学习的算法要取得好效果，离不开数据，咱们先把数据加载进来看看。\n",
    "\n",
    "#### 准备数据\n",
    "准备好数据，我们挑选 科技、汽车、娱乐、军事、运动 总共5类文本数据进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:56:12.954501Z",
     "start_time": "2022-09-02T04:56:10.227828Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "df_technology = pd.read_csv(\"/usr/local/codeData/jupyterData/jupyter-bd/textCategorization/data/technology_news.csv\", encoding='utf-8')\n",
    "df_technology = df_technology.dropna()\n",
    "\n",
    "df_car = pd.read_csv(\"/usr/local/codeData/jupyterData/jupyter-bd/textCategorization/data/car_news.csv\", encoding='utf-8')\n",
    "df_car = df_car.dropna()\n",
    "\n",
    "df_entertainment = pd.read_csv(\"/usr/local/codeData/jupyterData/jupyter-bd/textCategorization/data/entertainment_news.csv\", encoding='utf-8')\n",
    "df_entertainment = df_entertainment.dropna()\n",
    "\n",
    "df_military = pd.read_csv(\"/usr/local/codeData/jupyterData/jupyter-bd/textCategorization/data/military_news.csv\", encoding='utf-8')\n",
    "df_military = df_military.dropna()\n",
    "\n",
    "df_sports = pd.read_csv(\"/usr/local/codeData/jupyterData/jupyter-bd/textCategorization/data/sports_news.csv\", encoding='utf-8')\n",
    "df_sports = df_sports.dropna()\n",
    "\n",
    "technology = df_technology.content.values.tolist()[1000:21000]\n",
    "car = df_car.content.values.tolist()[1000:21000]\n",
    "entertainment = df_entertainment.content.values.tolist()[:20000]\n",
    "military = df_military.content.values.tolist()[:20000]\n",
    "sports = df_sports.content.values.tolist()[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:56:12.960778Z",
     "start_time": "2022-09-02T04:56:12.957159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　现在家里都拉了网线，都能无线上网，一定要帮他们先登上WiFi，另外，老人不懂得流量是什么，也不知道如何开关，控制流量，所以设置好流量上限很重要，免得不小心点开了视频或者下载，电话费就大发了。\n",
      "　　截至发稿时，人人车给出的处理方案仍旧是检修车辆。王先生则认为，车辆在购买时就存在问题，但交易平台并未能检测出来。因此，王先生希望对方退款。王先生称，他将找专业机构对车辆进行鉴定，并通过法律途径维护自己的权益。J256\n",
      "　　网综尺度相对较大原本是制作优势，《奇葩说》也曾经因为讨论的话题较为前卫一度引发争议。但《奇葩说》对于价值观的把握和引导让其中内含的“少儿不宜”只能算是小花絮。而纯粹是为了制造话题而“污”得“无节操无下限”的网综不仅让人生厌，也给节目自身招致了下架的厄运。对资本方而言，即便只从商业运营考量，点击量也分有价值和无价值，节目内容的变现能力如果建立在博眼球和低趣味迎合上，商业运营也难长久。对节目制作方与平台来说，为博一时的高点击而不顾底线不仅是砸自己招牌，以噱头吸引而来的观众与流量也是难以维持。\n",
      "　　央视记者 胡善敏：我现在所处的位置是在辽宁舰的飞行甲板，执行跨海区训练和试验任务的辽宁舰官兵，正在展开多个科目的训练，穿着不同颜色服装的官兵在紧张的对舰载机进行转运。\n",
      "　　据统计，2016年仅在中国田径协会注册的马拉松赛事便达到了328场，继续呈现出爆发式增长的态势，2015年，这个数字还仅仅停留在134场。如果算上未在中国田协注册的纯“民间”赛事，国内全年的路跑赛事还要更多。\n"
     ]
    }
   ],
   "source": [
    "print (technology[12])\n",
    "print (car[100])\n",
    "print (entertainment[10])\n",
    "print (military[10])\n",
    "print (sports[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词与中文文本处理\n",
    "#### 停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:56:12.970309Z",
     "start_time": "2022-09-02T04:56:12.962874Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords=pd.read_csv(\"/usr/local/codeData/jupyterData/jupyter-bd/textCategorization/data/stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "stopwords=stopwords['stopword'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:59:26.083458Z",
     "start_time": "2022-09-02T04:56:12.972149Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.737 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(content_lines, sentences, category):\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs=jieba.lcut(line)\n",
    "            segs = filter(lambda x:len(x)>1, segs)\n",
    "            segs = filter(lambda x:x not in stopwords, segs)\n",
    "            sentences.append((\" \".join(segs), category))\n",
    "        except Exception:\n",
    "            print (line)\n",
    "            continue \n",
    "\n",
    "#生成训练数据\n",
    "sentences = []\n",
    "\n",
    "preprocess_text(technology, sentences, 'technology')\n",
    "preprocess_text(car, sentences, 'car')\n",
    "preprocess_text(entertainment, sentences, 'entertainment')\n",
    "preprocess_text(military, sentences, 'military')\n",
    "preprocess_text(sports, sentences, 'sports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:59:26.132956Z",
     "start_time": "2022-09-02T04:59:26.086627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国 介绍 世界 entertainment\n",
      "精准 用户 知晓 兴趣 购票 转化 口碑 转化成 IP 品牌价值 沉淀 二次 循环 entertainment\n",
      "手动 休假 出征 military\n",
      "被誉为 北疆 第一 伊木河 哨所 地处 大兴安岭 深处 冬季 奇寒 漫长 无霜期 多天 最低气温 零下 摄氏度 称为 中国 最冷 地方 military\n",
      "广东 深圳 滨海 滨河 大道 启用 首条 乘员 车辆 专用车 载客 车辆 启用 时间段 工作日 早晚 高峰 car\n",
      "竣工 仪式 尾声 基金会 企业 志愿者 孩子 文具 体育用品 郑海霞 同学 赠送 签名 篮球 勉励 孩子 好好学习 文化 知识 好好 锻炼身体 努力 祖国 栋梁 回报 社会 sports\n",
      "2012 青海湖 景区 青海 市民 群众 推出 以爱为 主题 元旦 徒步 迎新年 活动 传达 低碳 环保 户外 旅游 理念 参与 人数 逐年 增加 公里 徒步 过程 捡拾 垃圾 公益活动 sports\n",
      "胡先生 平时 上班 之余 微信 卖片 胡先生 来到 辖区 派出所 报警 银行卡 1818 不到 一分钟 买家 没收 民警 调查 发现 当日 时许 胡先生 接到 陌生人 发来 微信 说要 买部 电影 商议 价格 买家 扫描 二维码 付款 胡先生 没多想 微信 钱包 首页 二维码 条形码 截图 发给 买家 系统 延迟 截图 没收 胡先生 连发 三次 不到 一分钟 胡先生 收到 银行 告知 短信 银行卡 被扣 1818 胡先生 赶紧 买家 发现 微信 拉黑 technology\n",
      "学习 强手 学习 强手 冰雪 运动 布局 我国 冰雪 运动 发展 偏科 现象 短道 速滑 花样滑冰 双人滑 冰壶 自由式 滑雪 空中 技巧 项目 奠定 优势 冰上 项目 项目 欠账 中国 冰雪 运动 想要 世界 占有 一席之地 改变 均衡 布局 项目 世界 内有 影响 项目 突破 冰雪 运动 布局 日本 韩国 哈萨克斯坦 值得 借鉴 sports\n",
      "具备 下水 条件 military\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(sentences)\n",
    "\n",
    "for sentence in sentences[:10]:\n",
    "    print (sentence[0], sentence[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了一会儿检测一下咱们的分类器效果怎么样，我们需要一份测试集。\n",
    "\n",
    "所以把原数据集分成训练集的测试集，咱们用sklearn自带的分割函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:59:26.903069Z",
     "start_time": "2022-09-02T04:59:26.135129Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x, y = zip(*sentences)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:59:26.915428Z",
     "start_time": "2022-09-02T04:59:26.906341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65696"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步要做的就是在降噪数据上抽取出来有用的特征啦，我们对文本抽取词袋模型特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:59:29.025385Z",
     "start_time": "2022-09-02T04:59:26.917337Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word', # tokenise by character ngrams\n",
    "    max_features=4000,  # keep the most common 1000 ngrams\n",
    ")\n",
    "vec.fit(x_train)\n",
    "\n",
    "def get_features(x):\n",
    "    vec.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:59:29.033065Z",
     "start_time": "2022-09-02T04:59:29.027242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=4000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把分类器import进来并且训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:59:30.679064Z",
     "start_time": "2022-09-02T04:59:29.034902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(x_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看我们的准确率如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:59:31.241153Z",
     "start_time": "2022-09-02T04:59:30.680831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8404036714005205"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(vec.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T04:59:31.247130Z",
     "start_time": "2022-09-02T04:59:31.243407Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21899"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到在2w多个样本上，我们能在5个类别上达到83的准确率。\n",
    "\n",
    "有没有办法把准确率提高一些呢？\n",
    "\n",
    "我们可以把特征做得更棒一点，比如说，我们试试加入抽取2-gram和3-gram的统计特征，比如可以把词库的量放大一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T05:00:06.738834Z",
     "start_time": "2022-09-02T04:59:31.249585Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word', # tokenise by character ngrams\n",
    "    ngram_range=(1,4),  # use ngrams of size 1 and 2\n",
    "    max_features=20000,  # keep the most common 1000 ngrams\n",
    ")\n",
    "vec.fit(x_train)\n",
    "\n",
    "def get_features(x):\n",
    "    vec.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分类训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T05:00:14.450704Z",
     "start_time": "2022-09-02T05:00:06.742398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8789442440294077"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(x_train), y_train)\n",
    "classifier.score(vec.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更可靠的验证效果的方式是交叉验证，但是交叉验证最好保证每一份里面的样本类别也是相对均衡的，我们这里使用StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T05:00:22.797638Z",
     "start_time": "2022-09-02T05:00:14.452738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8805366237978403\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "def stratifiedkfold_cv(x, y, clf_class, shuffle=True, n_folds=5, **kwargs):\n",
    "    #stratifiedk_fold = StratifiedKFold(y, n_folds=n_folds, shuffle=shuffle)\n",
    "    stratifiedk_fold = StratifiedKFold(n_splits=5)\n",
    "    y_pred = y[:]\n",
    "    for train_index, test_index in stratifiedk_fold.split(x, y):\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "    return y_pred \n",
    "\n",
    "NB = MultinomialNB\n",
    "print (precision_score(y, stratifiedkfold_cv(vec.transform(x),np.array(y),NB), average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们做完K折的交叉验证，可以看到在5个类别上的结果平均准确度约为88%\n",
    "\n",
    "### 我们自己来完成一个文本分类器class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T05:00:22.804743Z",
     "start_time": "2022-09-02T05:00:22.799550Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "class TextClassifier():\n",
    "\n",
    "    def __init__(self, classifier=MultinomialNB()):\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,4), max_features=20000)\n",
    "\n",
    "    def features(self, X):\n",
    "        return self.vectorizer.transform(X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.classifier.fit(self.features(X), y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.classifier.predict(self.features([x]))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.classifier.score(self.features(X), y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T05:00:58.755135Z",
     "start_time": "2022-09-02T05:00:22.806752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['military']\n",
      "0.8789442440294077\n"
     ]
    }
   ],
   "source": [
    "text_classifier = TextClassifier()\n",
    "text_classifier.fit(x_train, y_train)\n",
    "print(text_classifier.predict('这 是 有史以来 最 大 的 一 次 军舰 演习'))\n",
    "print(text_classifier.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM文本分类\n",
    "我们来试试支持向量机的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T05:14:41.633821Z",
     "start_time": "2022-09-02T05:00:58.759568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8494908443307914"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(vec.transform(x_train), y_train)\n",
    "svm.score(vec.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以试试rbf核"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T05:52:50.205015Z",
     "start_time": "2022-09-02T05:14:41.635854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8488515457326818"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(vec.transform(x_train), y_train)\n",
    "svm.score(vec.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 换特征/模型试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T05:52:50.212065Z",
     "start_time": "2022-09-02T05:52:50.207095Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "class TextClassifier():\n",
    "\n",
    "    def __init__(self, classifier=SVC(kernel='linear')):\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3), max_features=12000)\n",
    "\n",
    "    def features(self, X):\n",
    "        return self.vectorizer.transform(X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.classifier.fit(self.features(X), y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.classifier.predict(self.features([x]))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.classifier.score(self.features(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-02T06:05:50.991490Z",
     "start_time": "2022-09-02T05:52:50.214028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['military']\n",
      "0.877893967761085\n"
     ]
    }
   ],
   "source": [
    "text_classifier = TextClassifier()\n",
    "text_classifier.fit(x_train, y_train)\n",
    "print(text_classifier.predict('这 是 有史以来 最 大 的 一 次 军舰 演习'))\n",
    "print(text_classifier.score(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
