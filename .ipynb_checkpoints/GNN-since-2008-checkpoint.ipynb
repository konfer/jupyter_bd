{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/Steboss89/b21d6abe548d106119666fec6b65965f/publicgat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ipfiA6lc0g9"
   },
   "source": [
    "### CPU implementation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-vm-Ifyf_kW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as funct\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    r\"\"\" Main GAT class implementing Bahdanau's Attention\"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout=0.6, concat_output=True):\n",
    "        r\"\"\" Constructor\n",
    "        Define input and output sizes\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features: int, input size\n",
    "        out_features: int, output size\n",
    "        dropout: float, dropout rate \n",
    "        concat_output: Bool, default True, concatenate attentions' outputs\n",
    "        \"\"\"\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.concat_output = concat_output\n",
    "        self.dropout = dropout\n",
    "    \n",
    "        # initialize weight matrix W \n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data)\n",
    "        # initialize attention nnet 'a'--> 2 layers and output is 1\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data)\n",
    "        \n",
    "    def forward(self, h, adjacency):\n",
    "        r\"\"\" Define the forward step where the attention is computed\n",
    "        Parameters\n",
    "        ----------\n",
    "        h: np.array input nodes' features\n",
    "        adjacency: np.array, input nodes' adjacency matrix \n",
    "        \"\"\"\n",
    "        # first compute W*h\n",
    "        WH = torch.matmul(h, self.W) # size N (nodes) * out_features\n",
    "        # compute WH*first layer from neural network a \n",
    "        # for info check\n",
    "        # https://towardsdatascience.com/graph-neural-networks-a-learning-journey-since-2008-graph-attention-networks-f8c39189e7fc#feee\n",
    "        # take first layer \n",
    "        WH1 = torch.matmul(WH, self.a[:self.out_features, :])\n",
    "        # take second layer \n",
    "        WH2 = torch.matmul(WH, self.a[self.out_features:, :])\n",
    "        # sum up everything to compute the alignment score \n",
    "        e = nn.LeakyReLU(0.2)(WH1 + WH2.T)\n",
    "        # here we gather adjacency matrix with e\n",
    "        # to avoid overflow substitue 0 values with -1e9\n",
    "        # and where adjacency values > 0 use e values\n",
    "        attnt = torch.where(adjacency > 0, e, -1e9*torch.ones_like(e) ) \n",
    "        # compute the softmax \n",
    "        attnt = funct.softmax(attnt, dim=1)\n",
    "        # add the dropout step. self.training is a boolean define if we're at training time\n",
    "        attnt = funct.dropout(attnt, self.dropout, training=self.training)\n",
    "        # at this point we compute the new nodes' features representation h'\n",
    "        hfirst = torch.matmul(attnt, WH)\n",
    "        # here we can concatenate the output or \n",
    "        # return the final hrist \n",
    "        if self.concat_output:\n",
    "            return funct.elu(hfirst)\n",
    "        else:\n",
    "            return hfirst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FI8ikghogYkY"
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    r\"\"\" Main GAT model, ready to be trained, with multi head attention\"\"\"\n",
    "    def __init__(self, in_features, out_features, nclass, nheads, dropout=0.6):\n",
    "        r\"\"\" Constructore for GAT \n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features: int, number of input features \n",
    "        out_features: int, number of output features from attention layers \n",
    "        nclass: int, total number of class to be predicted \n",
    "        nheads: int, number of attention heads\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        # now create all the attention heads\n",
    "        self.attheads = [GraphAttentionLayer(in_features, out_features, concat_output=True) for _ in range(nheads)]\n",
    "        # output is a final attention without concat, which takes as input all the previous outputs \n",
    "        self.output = GraphAttentionLayer(out_features*nheads, nclass, concat_output=False)\n",
    "\n",
    "        \n",
    "    def forward(self, X, adjacency):\n",
    "        r\"\"\" Main forwards step\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: np.array, input nodes' featuers \n",
    "        adjacency: np.array, adjacency matrix \n",
    "        \"\"\"\n",
    "        X = funct.dropout(X, self.dropout, training=self.training)\n",
    "        # compute the attention from each head given the input \n",
    "        X = torch.cat([attn(X, adjacency) for attn in self.attheads], dim=1) \n",
    "        # dropout \n",
    "        x = funct.dropout(X, self.dropout, training=self.training)\n",
    "        # ELU activation \n",
    "        X = funct.elu(self.output(X, adjacency))\n",
    "        # return the final result \n",
    "        return funct.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Ok7CeO5gc_G",
    "outputId": "e42d03c0-513e-411e-f120-6bb8011ce21c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
      "172032/168052 [==============================] - 0s 1us/step\n",
      "180224/168052 [================================] - 0s 1us/step\n",
      "{'Neural_Networks': array([1., 0., 0., 0., 0., 0., 0.]), 'Reinforcement_Learning': array([0., 1., 0., 0., 0., 0., 0.]), 'Theory': array([0., 0., 1., 0., 0., 0., 0.]), 'Rule_Learning': array([0., 0., 0., 1., 0., 0., 0.]), 'Case_Based': array([0., 0., 0., 0., 1., 0., 0.]), 'Genetic_Algorithms': array([0., 0., 0., 0., 0., 1., 0.]), 'Probabilistic_Methods': array([0., 0., 0., 0., 0., 0., 1.])}\n",
      "Train elements 140\n",
      "Validation elements 300\n",
      "Test elements 1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from tensorflow import keras \n",
    "import os\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    r\"\"\" Transform labels into a one hot encoded vector\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels: np.array, this is the vector of labels \n",
    "\n",
    "    Return \n",
    "    ------\n",
    "    labels_onehot: np.array, one hot encoded label \n",
    "    \"\"\"\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    print(classes_dict)\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    r\"\"\" Function to normalize values of a given sparse array mx\n",
    "    Parameters\n",
    "    ----------\n",
    "    mx: scipy.sparse.coo_matrix, input sparse matrix to be normalized \n",
    "\n",
    "    Return \n",
    "    ------\n",
    "    mx: scipy.sparse.coo_matrix, normalize array\n",
    "    \"\"\"\n",
    "\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    r\"\"\" Function to load the Cora dataset. \n",
    "    The Cora dataset is downloaded thorugh tensorflow keras. Nodes' features \n",
    "    and adjacency matrix are created from cora.content and cora.cites\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download file\n",
    "    zip_file = keras.utils.get_file(\n",
    "        fname=\"cora.tgz\",\n",
    "        origin=\"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\",\n",
    "        extract=True,\n",
    "    )\n",
    "    # create the path\n",
    "    data_dir = os.path.join(os.path.dirname(zip_file), \"cora\")\n",
    "\n",
    "    # content data is converted to numpy vector\n",
    "    idx_features_labels = np.genfromtxt(f\"{data_dir}/cora.content\", dtype=np.dtype(str))\n",
    "    \n",
    "    # Take the bag-of-words vector of each paper as the feature vector of each article and store it in a sparse matrix format\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    \n",
    "    # Take the type of each paper as a label and convert it into a one hot vector\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # Take out the id of each paper\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    \n",
    "    # cites data is converted to numpy vector\n",
    "    edges_unordered = np.genfromtxt(f\"{data_dir}/cora.cites\",dtype=np.int32)\n",
    "    \n",
    "    # Map the id in the cites data to the interval [0, 2708]\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    \n",
    "    # Store the citation relationship between papers in a sparse matrix format\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "    \n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    \n",
    "    # Normalize the characteristics of the article\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    \n",
    "    # Produce the final vector\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500) \n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = torch.LongTensor(adj.todense())\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    edge_tensor = torch.LongTensor(edges)\n",
    "\n",
    "    print(f\"Train elements {len(idx_train)}\")\n",
    "    print(f\"Validation elements {len(idx_val)}\")\n",
    "    print(f\"Test elements {len(idx_test)}\")\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test, edge_tensor\n",
    "\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test, edge_tensor = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEUv6LZQglhn"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "seed = 42\n",
    "epochs = 100\n",
    "lr = 0.005 \n",
    "weight_decay = 5e-4 \n",
    "hidden = 8 \n",
    "heads = 8 \n",
    "\n",
    "model = GAT(in_features = features.shape[1], \n",
    "            out_features = hidden,\n",
    "            nclass=7,\n",
    "            nheads = heads)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=lr, \n",
    "                       weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BDwcYBpbkyi3"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    r\"\"\" This function train the GAT model. The function works through variables\n",
    "    defined in the code - but we can refactor this function to work anyway - \n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch: int, current epoch \n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = funct.nll_loss(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # validation \n",
    "    loss_val = funct.nll_loss(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    if epoch%10==0:\n",
    "        print(f'Epoch: {epoch+1}'.format(epoch+1),\n",
    "            f'loss_train: {loss_train.data.item()}'\n",
    "            f'loss_val: {loss_val.data.item()}',\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wDjog6nlhi2",
    "outputId": "fb35a628-ce98-4c59-eb25-5a462ad60dfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss_train: 1.945987343788147loss_val: 1.9458523988723755\n",
      "Epoch: 11 loss_train: 1.943637490272522loss_val: 1.9434393644332886\n",
      "Epoch: 21 loss_train: 1.941507339477539loss_val: 1.9412577152252197\n",
      "Epoch: 31 loss_train: 1.9395989179611206loss_val: 1.9393465518951416\n",
      "Epoch: 41 loss_train: 1.9375416040420532loss_val: 1.9372817277908325\n",
      "Epoch: 51 loss_train: 1.9359586238861084loss_val: 1.9355454444885254\n",
      "Epoch: 61 loss_train: 1.9342877864837646loss_val: 1.9336856603622437\n",
      "Epoch: 71 loss_train: 1.9333196878433228loss_val: 1.9326894283294678\n",
      "Epoch: 81 loss_train: 1.9325860738754272loss_val: 1.9318381547927856\n",
      "Epoch: 91 loss_train: 1.9313238859176636loss_val: 1.9303865432739258\n",
      "CPU times: user 2min 46s, sys: 8.12 s, total: 2min 55s\n",
      "Wall time: 2min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOTGne6oc8aM"
   },
   "source": [
    "### CPU implementation with torch geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WwqVRrXwc-v4",
    "outputId": "ccbb6460-25df-4d54-e3c7-0a8ac8857baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
      "Collecting torch-scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.9 MB 5.4 MB/s \n",
      "\u001b[?25hInstalling collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-2.0.9\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
      "Collecting torch-sparse\n",
      "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 5.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n",
      "Installing collected packages: torch-sparse\n",
      "Successfully installed torch-sparse-0.6.13\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
      "Collecting torch-cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 5.2 MB/s \n",
      "\u001b[?25hInstalling collected packages: torch-cluster\n",
      "Successfully installed torch-cluster-1.6.0\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
      "Collecting torch-spline-conv\n",
      "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (750 kB)\n",
      "\u001b[K     |████████████████████████████████| 750 kB 4.3 MB/s \n",
      "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
      "Successfully installed torch-spline-conv-1.2.1\n",
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
      "\u001b[K     |████████████████████████████████| 407 kB 5.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
      "Building wheels for collected packages: torch-geometric\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=e24de4c4293e72eb42346ce8424b71c54e5490f0fc0f63118900b074105a0272\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
      "Successfully built torch-geometric\n",
      "Installing collected packages: torch-geometric\n",
      "Successfully installed torch-geometric-2.0.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def format_pytorch_version(version):\n",
    "  return version.split('+')[0]\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "def format_cuda_version(version):\n",
    "  return 'cu' + version.replace('.', '')\n",
    "\n",
    "CUDA_version = torch.version.cuda\n",
    "CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-geometric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p7j1bDHZZ1oc",
    "outputId": "6ff93dd5-c213-4dcd-e554-ec867324a477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 25 09:27:57 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   69C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MjFaMg8dCVq",
    "outputId": "0fdfc05b-9ea3-4ee7-e7c5-6896c1159ea0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as funct \n",
    "\n",
    "from torch_geometric.data import Data \n",
    "from torch_geometric.nn import GATConv \n",
    "from torch_geometric.datasets import Planetoid \n",
    "import torch_geometric.transforms as T \n",
    "\n",
    "dataset = Planetoid(root=\"/tmp/Cora\", name=\"Cora\")\n",
    "dataset.transform = T.NormalizeFeatures() \n",
    "#print(f\"Number of Classes in:\", dataset.num_classes)\n",
    "#print(f\"Number of Node Features in:\", dataset.num_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jraQ3VXtem3K",
    "outputId": "d1304e6e-6bf4-4300-ddf8-7007b2e9069b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\n",
      "{'Neural_Networks': array([1., 0., 0., 0., 0., 0., 0.]), 'Theory': array([0., 1., 0., 0., 0., 0., 0.]), 'Reinforcement_Learning': array([0., 0., 1., 0., 0., 0., 0.]), 'Probabilistic_Methods': array([0., 0., 0., 1., 0., 0., 0.]), 'Genetic_Algorithms': array([0., 0., 0., 0., 1., 0., 0.]), 'Rule_Learning': array([0., 0., 0., 0., 0., 1., 0.]), 'Case_Based': array([0., 0., 0., 0., 0., 0., 1.])}\n",
      "Train elements 140\n",
      "Validation elements 300\n",
      "Test elements 1000\n"
     ]
    }
   ],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    r\"\"\" Main class for GAT\"\"\"\n",
    "    def __init__(self, in_features, out_features, nclass, nheads): # in_features, out_features, nclass, nheads):\n",
    "        r\"\"\" Constructor, define input features, output features, nclass and nheads \n",
    "        for attention layer\"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.hid = out_features\n",
    "        self.in_head = nheads\n",
    "        self.out_head = out_features\n",
    "        self.nclass = nclass\n",
    "        \n",
    "        #GATConv(in_channels, out_channels, heads, concat)\n",
    "        # This is the multi-head layer so concat outputs\n",
    "        self.conv1 = GATConv(in_features,\n",
    "                             self.hid,\n",
    "                             heads=self.in_head,\n",
    "                             dropout=0.6)\n",
    "        # here we want to average\n",
    "        # in_feautres = output from self.conv1 (self.hid)*number of heads\n",
    "        # out features --> number of classes to predict\n",
    "        self.conv2 = GATConv(self.hid*self.in_head,\n",
    "                             self.nclass,\n",
    "                             concat=False,\n",
    "                             heads=self.out_head,\n",
    "                             dropout=0.6)\n",
    "\n",
    "    def forward(self, X, adjacency):\n",
    "        r\"\"\" forward step for training\"\"\"\n",
    "        x, edge_index = X, adjacency\n",
    "                \n",
    "        x = funct.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = funct.elu(x)\n",
    "        x = funct.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return funct.log_softmax(x, dim=1)\n",
    "\n",
    "hidden = 8 \n",
    "nheads = 8\n",
    "nclass = dataset.num_classes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device {device}\")\n",
    "\n",
    "# data \n",
    "adj, features, labels, idx_train, idx_val, idx_test, edge_tensor = load_data()\n",
    "# send model to device \n",
    "model = GAT(in_features = features.shape[1],#dataset.num_features, \n",
    "            out_features = hidden,\n",
    "            nclass = 7, #nclass,\n",
    "            nheads = nheads).to(device)\n",
    "#data = dataset[0].to(device)\n",
    "\n",
    "# define optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FYXBrAmanJzS",
    "outputId": "5258bbaa-08a1-4ebe-89b7-d3bd3136ca0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss_train: 1.9463061094284058loss_val: 1.945523738861084\n",
      "Epoch: 11 loss_train: 1.8703289031982422loss_val: 1.8736236095428467\n",
      "Epoch: 21 loss_train: 1.7788177728652954loss_val: 1.7778674364089966\n",
      "Epoch: 31 loss_train: 1.7344424724578857loss_val: 1.7231016159057617\n",
      "Epoch: 41 loss_train: 1.6397923231124878loss_val: 1.7029311656951904\n",
      "Epoch: 51 loss_train: 1.554422378540039loss_val: 1.6239001750946045\n",
      "Epoch: 61 loss_train: 1.446123480796814loss_val: 1.5282834768295288\n",
      "Epoch: 71 loss_train: 1.3288929462432861loss_val: 1.4940941333770752\n",
      "Epoch: 81 loss_train: 1.2511863708496094loss_val: 1.4255460500717163\n",
      "Epoch: 91 loss_train: 1.1075749397277832loss_val: 1.3526391983032227\n",
      "CPU times: user 9.44 s, sys: 139 ms, total: 9.58 s\n",
      "Wall time: 9.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, edge_tensor.T)#model(data.x, data.edge_index)\n",
    "    loss =  funct.nll_loss(output[idx_train], labels[idx_train])#funct.nll_loss(output[data.train_mask], data.y[data.train_mask])\n",
    "    # validation \n",
    "    loss_val = funct.nll_loss(output[idx_val], labels[idx_val])\n",
    "\n",
    "    if epoch%10==0:\n",
    "        print(f'Epoch: {epoch+1}'.format(epoch+1),\n",
    "            f'loss_train: {loss.data.item()}'\n",
    "            f'loss_val: {loss_val.data.item()}',\n",
    "            )\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzbDbaasocsO"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTPopiLwSnkh"
   },
   "source": [
    "# GAT in JAX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ntg1cnjzl633"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import jax.numpy as np\n",
    "from jax import lax, random\n",
    "from jax.nn.initializers import glorot_normal, glorot_uniform\n",
    "import jax.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isEEz7Q-_DcM",
    "outputId": "fce91d84-4571-4d76-9931-0c0250d29d6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 28 11:22:49 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   73C    P0    75W / 149W |  10442MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wx8GFFstSoms"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import jax.numpy as np\n",
    "from jax import lax, random\n",
    "from jax.nn.initializers import glorot_normal, glorot_uniform\n",
    "import jax.nn as nn\n",
    "from jax import jit\n",
    "import numpy.random as npr\n",
    "\n",
    "@jit \n",
    "def create_random():\n",
    "    r\"\"\" Use this function to speed up the creation of random nunmbers\n",
    "\n",
    "    Return \n",
    "    ------\n",
    "    jax.random.PRNGKey\n",
    "    \"\"\"\n",
    "    return random.split(random.PRNGKey(npr.randint(0,100)), 4)\n",
    "\n",
    "\n",
    "def Dropout(rate):\n",
    "    r\"\"\" Layer construction function for a dropout layer with given rate.\n",
    "    This Dropout layer is modified from stax.experimental.Dropout, to use\n",
    "    `is_training` as an argument to apply_fun, instead of defining it at\n",
    "    definition time.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    rate: float, probability of keeping an element\n",
    "\n",
    "    Return \n",
    "    ------\n",
    "    init_fun: initializer function \n",
    "    apply_fun: application function\n",
    "    \"\"\"\n",
    "\n",
    "    def init_fun(input_shape):\n",
    "        r\"\"\" Constructor\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape: input dimension\n",
    "        \"\"\"\n",
    "        return input_shape, ()\n",
    "\n",
    "    def apply_fun(inputs, is_training, **kwargs):\n",
    "        r\"\"\" Function to compute dropout \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs: input features \n",
    "        is_training: bool, if the model is in training\n",
    "\n",
    "        Return \n",
    "        ------\n",
    "        out: lax.condition \n",
    "        \"\"\"\n",
    "        # generate a random number generate a bernoulli prob\n",
    "        rng, rng2, rng3, rng4 = create_random()\n",
    "        # keep rate\n",
    "        keep = random.bernoulli(rng,  1.0 - rate, inputs.shape)\n",
    "        # output that is kept from input features\n",
    "        outs = keep*inputs/(1.0 -rate) \n",
    "        # if not training, just return inputs and discard any computation done\n",
    "        out = lax.cond(is_training, outs, lambda x: x, inputs, lambda x: x)\n",
    "        return out\n",
    "\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "\n",
    "def GraphAttentionLayer(out_dim, dropout):\n",
    "    r\"\"\" Main layer for graph attention \n",
    "    Parameters\n",
    "    ----------\n",
    "    out_dim: output dimension \n",
    "    dropout: float, dropout rate \n",
    "    \"\"\"\n",
    "    _, drop_fun = Dropout(dropout)\n",
    "\n",
    "    def init_fun(input_shape):\n",
    "        r\"\"\" Constructor, generate input weights\"\"\"\n",
    "        output_shape = input_shape[:-1] + (out_dim,)\n",
    "        k1, k2, k3, k4 = create_random()\n",
    "        # initialize weight\n",
    "        W = glorot_uniform()(k1, (input_shape[-1], out_dim))\n",
    "        # initialize nn weight\n",
    "        a_init = glorot_uniform()\n",
    "        a1 = a_init(k2, (out_dim, 1))\n",
    "        a2 = a_init(k3, (out_dim, 1))\n",
    "\n",
    "        return output_shape, (W, a1, a2)\n",
    "       \n",
    "    def apply_fun(params, x, adj, activation=nn.elu, is_training=False, **kwargs):\n",
    "        r\"\"\"Apply function, compute the attention \n",
    "        Parameters\n",
    "        ----------\n",
    "        params: input parameters for weights W, a1, a2\n",
    "        x: input nodes'features\n",
    "        adj: adjacency matrix \n",
    "        \"\"\"\n",
    "        W, a1, a2 = params\n",
    "        # initial dropout\n",
    "        x = drop_fun(x, is_training=is_training)\n",
    "        # weights matmult\n",
    "        x = np.dot(x, W)\n",
    "        # neural netw + alignment score\n",
    "        f_1 = np.dot(x, a1) \n",
    "        f_2 = np.dot(x, a2)\n",
    "        logits = f_1 + f_2.T\n",
    "        # softmax of leakyReLu for e\n",
    "        coefs = nn.softmax( nn.leaky_relu(logits, negative_slope=0.2) + np.where(adj, 0., -1e9))\n",
    "        # final dropout\n",
    "        coefs = drop_fun(coefs, is_training=is_training)\n",
    "        x = drop_fun(x, is_training=is_training)\n",
    "\n",
    "        ret = np.matmul(coefs, x)\n",
    "\n",
    "        return activation(ret)\n",
    "\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "\n",
    "def MultiHeadLayer(nheads: int, nhid: int, dropout: float,last_layer: bool=False):\n",
    "    r\"\"\" Multi head attention layer\n",
    "    Parameters\n",
    "    ----------\n",
    "    nheads: int, number of attention heads \n",
    "    nhid: int, number of hidden units \n",
    "    dropout: float, percentage of dropout\n",
    "    last_layer: bool, if last lyer average, otherwise concat\n",
    "    \"\"\"\n",
    "    \n",
    "    layer_funs, layer_inits = [], []\n",
    "    # define the heads layers\n",
    "    for head_i in range(nheads):\n",
    "        att_init, att_fun = GraphAttentionLayer(nhid, dropout=dropout)\n",
    "        # initialize layers of attention\n",
    "        layer_inits.append(att_init)\n",
    "        # grab the functions for running attentions\n",
    "        layer_funs.append(att_fun)\n",
    "    \n",
    "    def init_fun(input_shape):\n",
    "        r\"\"\" Initialize each attention head\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shapee: int, input shape\n",
    "\n",
    "        Return \n",
    "        ------\n",
    "        input_shape: int, input shape \n",
    "        params: list, list of parameters for each attention head\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        # for each head initialize parameters\n",
    "        for att_init_fun in layer_inits:\n",
    "            #rng, layer_rng = random.split(rng)\n",
    "            layer_shape, param = att_init_fun(input_shape)\n",
    "            params.append(param)\n",
    "\n",
    "        input_shape = layer_shape\n",
    "        if not last_layer:\n",
    "            # multiply by the number of heads\n",
    "            input_shape = input_shape[:-1] + (input_shape[-1]*len(layer_inits),)\n",
    "        return input_shape, params\n",
    "    \n",
    "    def apply_fun(params, x, adj, is_training=False, **kwargs):\n",
    "        r\"\"\" Function to apply parameters to head \n",
    "        Parameters\n",
    "        ----------\n",
    "        params: list, lis tof parameters for attention \n",
    "        x: array, input array\n",
    "        adj: array, input adjacency \n",
    "        is_training:  bool\n",
    "        \"\"\"\n",
    "\n",
    "        layer_outs = []\n",
    "        assert len(params) == nheads\n",
    "        for head_i in range(nheads):\n",
    "            layer_params = params[head_i]\n",
    "            layer_outs.append(layer_funs[head_i](layer_params, x, adj, is_training=is_training))\n",
    "        # concatenate or average\n",
    "        if not last_layer:\n",
    "            x = np.concatenate(layer_outs, axis=1)\n",
    "        else:\n",
    "            # average last layer heads\n",
    "            x = np.mean(np.stack(layer_outs), axis=0)\n",
    "\n",
    "        return x\n",
    "\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "\n",
    "def GAT(nheads: List[int], nhid: List[int], nclass: int, dropout: float):\n",
    "    \"\"\"\n",
    "    Graph Attention Network model definition.\n",
    "    \"\"\"\n",
    "\n",
    "    init_funs = []\n",
    "    attn_funs = []\n",
    "\n",
    "    nhid += [nclass]\n",
    "    for layer_i in range(len(nhid)):\n",
    "        last = layer_i == len(nhid) - 1\n",
    "        layer_init, layer_fun = MultiHeadLayer(nheads[layer_i], nhid[layer_i],dropout=dropout,last_layer=last)\n",
    "        attn_funs.append(layer_fun)\n",
    "        init_funs.append(layer_init)\n",
    "\n",
    "    def init_fun(input_shape):\n",
    "        params = []\n",
    "        for i, init_fun in enumerate(init_funs):\n",
    "            layer_shape, param = init_fun(input_shape)\n",
    "            params.append(param)\n",
    "            input_shape = layer_shape\n",
    "        return input_shape, params\n",
    "\n",
    "    def apply_fun(params, x, adj, is_training=False, **kwargs):\n",
    "\n",
    "        for i, layer_fun in enumerate(attn_funs):\n",
    "            x = layer_fun(params[i], x, adj, is_training=is_training)\n",
    "        \n",
    "        return nn.log_softmax(x)\n",
    "\n",
    "    return init_fun, apply_fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZvpBNf1p87a"
   },
   "outputs": [],
   "source": [
    "# recreate the load data funciton\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from tensorflow import keras \n",
    "import os\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    r\"\"\" Transform labels into a one hot encoded vector\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels: np.array, this is the vector of labels \n",
    "\n",
    "    Return \n",
    "    ------\n",
    "    labels_onehot: np.array, one hot encoded label \n",
    "    \"\"\"\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    print(classes_dict)\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    r\"\"\" Function to normalize values of a given sparse array mx\n",
    "    Parameters\n",
    "    ----------\n",
    "    mx: scipy.sparse.coo_matrix, input sparse matrix to be normalized \n",
    "\n",
    "    Return \n",
    "    ------\n",
    "    mx: scipy.sparse.coo_matrix, normalize array\n",
    "    \"\"\"\n",
    "\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    r\"\"\" Function to load the Cora dataset. \n",
    "    The Cora dataset is downloaded thorugh tensorflow keras. Nodes' features \n",
    "    and adjacency matrix are created from cora.content and cora.cites\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download file\n",
    "    zip_file = keras.utils.get_file(\n",
    "        fname=\"cora.tgz\",\n",
    "        origin=\"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\",\n",
    "        extract=True,\n",
    "    )\n",
    "    # create the path\n",
    "    data_dir = os.path.join(os.path.dirname(zip_file), \"cora\")\n",
    "\n",
    "    # content data is converted to numpy vector\n",
    "    idx_features_labels = np.genfromtxt(f\"{data_dir}/cora.content\", dtype=np.dtype(str))\n",
    "    \n",
    "    # Take the bag-of-words vector of each paper as the feature vector of each article and store it in a sparse matrix format\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    \n",
    "    # Take the type of each paper as a label and convert it into a one hot vector\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # Take out the id of each paper\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    \n",
    "    # cites data is converted to numpy vector\n",
    "    edges_unordered = np.genfromtxt(f\"{data_dir}/cora.cites\",dtype=np.int32)\n",
    "    \n",
    "    # Map the id in the cites data to the interval [0, 2708]\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    \n",
    "    # Store the citation relationship between papers in a sparse matrix format\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "    \n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    \n",
    "    # Normalize the characteristics of the article\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "    \n",
    "    # Produce the final vector\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = np.array(features.todense())\n",
    "\n",
    "    # JAX doesn't support sparse matrices yet\n",
    "    adj = np.asarray(adj.todense())\n",
    "\n",
    "    return adj, features, labels, np.array(idx_train), np.array(idx_val), np.array(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tgXfOCW5-IBx",
    "outputId": "9f956cea-e800-49df-80e4-1ec3173da941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Case_Based': array([1., 0., 0., 0., 0., 0., 0.]), 'Rule_Learning': array([0., 1., 0., 0., 0., 0., 0.]), 'Reinforcement_Learning': array([0., 0., 1., 0., 0., 0., 0.]), 'Neural_Networks': array([0., 0., 0., 1., 0., 0., 0.]), 'Probabilistic_Methods': array([0., 0., 0., 0., 1., 0., 0.]), 'Theory': array([0., 0., 0., 0., 0., 1., 0.]), 'Genetic_Algorithms': array([0., 0., 0., 0., 0., 0., 1.])}\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnCacgJfqIjh"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "from jax import jit, grad, random\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "@jit\n",
    "def loss(params, batch):\n",
    "    \"\"\"\n",
    "    The idxes of the batch indicate which nodes are used to compute the loss.\n",
    "    \"\"\"\n",
    "    inputs, targets, adj, is_training, idx = batch\n",
    "    preds = predict_fun(params, inputs, adj, is_training=is_training)\n",
    "    ce_loss = -np.mean(np.sum(preds[idx] * targets[idx], axis=1))\n",
    "    l2_loss = 5e-4 * optimizers.l2_norm(params)**2 # tf doesn't use sqrt\n",
    "    return ce_loss + l2_loss\n",
    "\n",
    "\n",
    "@jit\n",
    "def accuracy(params, batch):\n",
    "    inputs, targets, adj, is_training, idx = batch\n",
    "    target_class = np.argmax(targets, axis=1)\n",
    "    predicted_class = np.argmax(predict_fun(params, inputs, adj, \n",
    "        is_training=is_training), axis=1)\n",
    "    return np.mean(predicted_class[idx] == target_class[idx])\n",
    "\n",
    "\n",
    "@jit\n",
    "def loss_accuracy(params, batch):\n",
    "    inputs, targets, adj, is_training, idx = batch\n",
    "    preds = predict_fun(params, inputs, adj, is_training=is_training)\n",
    "    target_class = np.argmax(targets, axis=1)\n",
    "    predicted_class = np.argmax(preds, axis=1)\n",
    "    ce_loss = -np.mean(np.sum(preds[idx] * targets[idx], axis=1))\n",
    "    acc = np.mean(predicted_class[idx] == target_class[idx])\n",
    "    return ce_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pXPMVn2qMOt",
    "outputId": "df14a490-580e-45ce-89f6-16ce0326f1e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Iter 0/100 train_loss:1.9087, train_acc: 0.4500, val_loss:1.9120, val_acc: 0.4667\n",
      "Iter 10/100 train_loss:1.6392, train_acc: 0.5857, val_loss:1.6892, val_acc: 0.5200\n",
      "Iter 20/100 train_loss:1.7689, train_acc: 0.5286, val_loss:1.7940, val_acc: 0.4767\n",
      "Iter 30/100 train_loss:1.8067, train_acc: 0.5071, val_loss:1.8219, val_acc: 0.4833\n",
      "Iter 40/100 train_loss:1.8210, train_acc: 0.5571, val_loss:1.8353, val_acc: 0.5000\n",
      "Iter 50/100 train_loss:1.8317, train_acc: 0.5786, val_loss:1.8458, val_acc: 0.5067\n",
      "Iter 60/100 train_loss:1.8327, train_acc: 0.5214, val_loss:1.8463, val_acc: 0.4900\n",
      "Iter 70/100 train_loss:1.8372, train_acc: 0.5643, val_loss:1.8492, val_acc: 0.5133\n",
      "Iter 80/100 train_loss:1.8373, train_acc: 0.5214, val_loss:1.8487, val_acc: 0.5033\n",
      "Iter 90/100 train_loss:1.8403, train_acc: 0.5357, val_loss:1.8496, val_acc: 0.5133\n",
      "Test set acc: 0.39900001883506775\n",
      "CPU times: user 42.8 s, sys: 397 ms, total: 43.2 s\n",
      "Wall time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = 0.05\n",
    "num_epochs = 100\n",
    "n_nodes = adj.shape[0]\n",
    "n_feats = features.shape[1]\n",
    "\n",
    "# GAT params\n",
    "nheads = [8, 1]\n",
    "nhid = [8]\n",
    "dropout = 0.6 # probability of keeping\n",
    "residual = False\n",
    "\n",
    "init_fun, predict_fun = GAT(nheads=nheads,\n",
    "                            nhid=nhid,\n",
    "                            nclass=7,\n",
    "                            dropout=dropout,\n",
    "                            )\n",
    "\n",
    "input_shape = (-1, n_nodes, n_feats)\n",
    "_, init_params = init_fun(input_shape)\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "\n",
    "@jit\n",
    "def update(i, opt_state, batch):\n",
    "    params = get_params(opt_state)\n",
    "    return opt_update(i, grad(loss)(params, batch), opt_state)\n",
    "\n",
    "opt_state = opt_init(init_params)\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    batch = (features, labels, adj, True, idx_train)\n",
    "    opt_state = update(epoch, opt_state, batch)\n",
    "\n",
    "    params = get_params(opt_state)\n",
    "    eval_batch = (features, labels, adj, False, idx_val)\n",
    "    train_batch = (features, labels, adj, False, idx_train)\n",
    "    # additional step, everything can be loaded onto the GPU:\n",
    "    train_batch = jax.device_put(train_batch)\n",
    "    eval_batch = jax.device_put(eval_batch)\n",
    "    # without that we take about 1 min\n",
    "    train_loss, train_acc = loss_accuracy(params, train_batch)\n",
    "    val_loss, val_acc = loss_accuracy(params, eval_batch)\n",
    "    if epoch%10==0:\n",
    "        print((f\"Iter {epoch}/{num_epochs} train_loss:\"+\n",
    "            f\"{train_loss:.4f}, train_acc: {train_acc:.4f}, val_loss:\"+\n",
    "            f\"{val_loss:.4f}, val_acc: {val_acc:.4f}\"))\n",
    "\n",
    "    # new random key at each iteration, othwerwise dropout uses always \n",
    "    # the same mask \n",
    "\n",
    "# now run on the test set\n",
    "test_batch = (features, labels, adj, False, idx_test)\n",
    "test_acc = accuracy(params, test_batch)\n",
    "print(f'Test set acc: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Dy1ZHyjqgun"
   },
   "outputs": [],
   "source": [
    "# on Tesla T4 --> 17.2 seconds"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN0o7YvqJ6nFAibHR1ZU8dg",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "PublicGAT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
