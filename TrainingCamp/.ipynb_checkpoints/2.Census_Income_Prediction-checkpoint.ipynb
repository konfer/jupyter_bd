{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCI Adult数据集\n",
    "### 下载地址: https://archive.ics.uci.edu/ml/datasets/adult\n",
    "这是一个分类问题，我们要预测一个人的收入是否高于$50K/年，这个数据集也叫做\"Census收入\"数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import math\n",
    "from datetime import datetime\n",
    "import multiprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "from tensorflow.python.feature_column import feature_column\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'cenus-model-01'\n",
    "\n",
    "TRAIN_DATA_FILES_PATTERN = 'adult_train.csv'\n",
    "TEST_DATA_FILES_PATTERN = 'adult_test.csv'\n",
    "\n",
    "RESUME_TRAINING = False\n",
    "PROCESS_FEATURES = True\n",
    "EXTEND_FEATURE_COLUMNS = True\n",
    "MULTI_THREADING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义数据集的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全部列名: ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income_bracket']\n",
      "数值型的特征: ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
      "类别型的特征: ['gender', 'race', 'education', 'marital_status', 'relationship', 'workclass', 'occupation', 'native_country']\n",
      "目标列: income_bracket - 不同的分类结果: ['<=50K', '>50K']\n",
      "没有用到的列: []\n"
     ]
    }
   ],
   "source": [
    "# 数据集每个字段的名称\n",
    "HEADER = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "               'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "               'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "               'native_country', 'income_bracket']\n",
    "\n",
    "# 数据集默认值(数值型默认0，字符串型默认空串)\n",
    "HEADER_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
    "                       [0], [0], [0], [''], ['']]\n",
    "# 数值型的列\n",
    "NUMERIC_FEATURE_NAMES = ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "\n",
    "# 类别型的列，同时把列的不同取值列出来\n",
    "CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY = {\n",
    "    'gender': ['Female', 'Male'],\n",
    "    \n",
    "    'race': ['Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'],\n",
    "    \n",
    "    'education': ['Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college', \n",
    "                  'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school', \n",
    "                  '5th-6th', '10th', '1st-4th', 'Preschool', '12th'],\n",
    "    \n",
    "    'marital_status': ['Married-civ-spouse', 'Divorced', 'Married-spouse-absent', \n",
    "                       'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'],\n",
    "    \n",
    "    'relationship': ['Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried', 'Other-relative'],\n",
    "    \n",
    "    'workclass': ['Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov', 'Local-gov', '?', \n",
    "                  'Self-emp-inc', 'Without-pay', 'Never-worked']\n",
    "}\n",
    "\n",
    "# 类别比较多的，我们做hash分桶\n",
    "CATEGORICAL_FEATURE_NAMES_WITH_BUCKET_SIZE = {\n",
    "    'occupation': 50,\n",
    "    'native_country' : 100\n",
    "}\n",
    "\n",
    "# 类别型的列名\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.keys()) + list(CATEGORICAL_FEATURE_NAMES_WITH_BUCKET_SIZE.keys())\n",
    "\n",
    "# 总的列名\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "\n",
    "# 目标列名\n",
    "TARGET_NAME = 'income_bracket'\n",
    "\n",
    "# 目标不同类别的取值\n",
    "TARGET_LABELS = ['<=50K', '>50K']\n",
    "\n",
    "# 权重列\n",
    "WEIGHT_COLUMN_NAME = 'fnlwgt'\n",
    "\n",
    "# 没有用到的列\n",
    "UNUSED_FEATURE_NAMES = list(set(HEADER) - set(FEATURE_NAMES) - {TARGET_NAME} - {WEIGHT_COLUMN_NAME})\n",
    "\n",
    "\n",
    "print(\"全部列名: {}\".format(HEADER))\n",
    "print(\"数值型的特征: {}\".format(NUMERIC_FEATURE_NAMES))\n",
    "print(\"类别型的特征: {}\".format(CATEGORICAL_FEATURE_NAMES))\n",
    "print(\"目标列: {} - 不同的分类结果: {}\".format(TARGET_NAME, TARGET_LABELS))\n",
    "print(\"没有用到的列: {}\".format(UNUSED_FEATURE_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 做一点数据探索\n",
    "这里数据探索还是用的pandas，毕竟数据分析和自带的可视化，用这个会方便一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_bracket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education_num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital_status         occupation   relationship   race  gender  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week native_country income_bracket  \n",
       "0          2174             0              40  United-States          <=50K  \n",
       "1             0             0              13  United-States          <=50K  \n",
       "2             0             0              40  United-States          <=50K  \n",
       "3             0             0              40  United-States          <=50K  \n",
       "4             0             0              40           Cuba          <=50K  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(TRAIN_DATA_FILES_PATTERN, header=None, names=HEADER )\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      "age               32561 non-null int64\n",
      "workclass         32561 non-null object\n",
      "fnlwgt            32561 non-null int64\n",
      "education         32561 non-null object\n",
      "education_num     32561 non-null int64\n",
      "marital_status    32561 non-null object\n",
      "occupation        32561 non-null object\n",
      "relationship      32561 non-null object\n",
      "race              32561 non-null object\n",
      "gender            32561 non-null object\n",
      "capital_gain      32561 non-null int64\n",
      "capital_loss      32561 non-null int64\n",
      "hours_per_week    32561 non-null int64\n",
      "native_country    32561 non-null object\n",
      "income_bracket    32561 non-null object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education_num</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32561.000000</td>\n",
       "      <td>3.256100e+04</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.581647</td>\n",
       "      <td>1.897784e+05</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>1077.648844</td>\n",
       "      <td>87.303830</td>\n",
       "      <td>40.437456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.640433</td>\n",
       "      <td>1.055500e+05</td>\n",
       "      <td>2.572720</td>\n",
       "      <td>7385.292085</td>\n",
       "      <td>402.960219</td>\n",
       "      <td>12.347429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.178270e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.783560e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.370510e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.484705e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education_num  capital_gain  capital_loss  \\\n",
       "count  32561.000000  3.256100e+04   32561.000000  32561.000000  32561.000000   \n",
       "mean      38.581647  1.897784e+05      10.080679   1077.648844     87.303830   \n",
       "std       13.640433  1.055500e+05       2.572720   7385.292085    402.960219   \n",
       "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
       "25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000   \n",
       "50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000   \n",
       "75%       48.000000  2.370510e+05      12.000000      0.000000      0.000000   \n",
       "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
       "\n",
       "       hours_per_week  \n",
       "count    32561.000000  \n",
       "mean        40.437456  \n",
       "std         12.347429  \n",
       "min          1.000000  \n",
       "25%         40.000000  \n",
       "50%         40.000000  \n",
       "75%         45.000000  \n",
       "max         99.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_SIZE = train_data.shape[0]\n",
    "test_data = pd.read_csv(TEST_DATA_FILES_PATTERN, skiprows=1)\n",
    "TEST_DATA_SIZE = test_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Scaling Statistics for Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>stdv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>90</td>\n",
       "      <td>38.581647</td>\n",
       "      <td>17</td>\n",
       "      <td>13.640433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_num</th>\n",
       "      <td>16</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>1</td>\n",
       "      <td>2.572720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_gain</th>\n",
       "      <td>99999</td>\n",
       "      <td>1077.648844</td>\n",
       "      <td>0</td>\n",
       "      <td>7385.292085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_loss</th>\n",
       "      <td>4356</td>\n",
       "      <td>87.303830</td>\n",
       "      <td>0</td>\n",
       "      <td>402.960219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours_per_week</th>\n",
       "      <td>99</td>\n",
       "      <td>40.437456</td>\n",
       "      <td>1</td>\n",
       "      <td>12.347429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  max         mean  min         stdv\n",
       "age                90    38.581647   17    13.640433\n",
       "education_num      16    10.080679    1     2.572720\n",
       "capital_gain    99999  1077.648844    0  7385.292085\n",
       "capital_loss     4356    87.303830    0   402.960219\n",
       "hours_per_week     99    40.437456    1    12.347429"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = train_data[NUMERIC_FEATURE_NAMES].mean(axis=0)\n",
    "stdvs = train_data[NUMERIC_FEATURE_NAMES].std(axis=0)\n",
    "maxs = train_data[NUMERIC_FEATURE_NAMES].max(axis=0)\n",
    "mins = train_data[NUMERIC_FEATURE_NAMES].min(axis=0)\n",
    "df_stats = pd.DataFrame({\"mean\":means, \"stdv\":stdvs, \"max\":maxs, \"min\":mins})\n",
    "df_stats.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 存储统计分析数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats.to_csv(path_or_buf=\"adult.stats.csv\", header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整体结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7xo0y8.com1.z0.glb.clouddn.com/tf/estimator.png?imageView2/2/w/400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义数据输入函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. 解析csv与预处理逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_row(csv_row):\n",
    "    # help(tf.decode_csv)\n",
    "    columns = tf.decode_csv(csv_row, record_defaults=HEADER_DEFAULTS)\n",
    "    # 把tensor和对应的列名打包成字典\n",
    "    features = dict(zip(HEADER, columns))\n",
    "    # 去除无用的列\n",
    "    for column in UNUSED_FEATURE_NAMES:\n",
    "        features.pop(column)\n",
    "    # 取出目标列\n",
    "    target = features.pop(TARGET_NAME)\n",
    "    # 返回 字典+target序列形式\n",
    "    return features, target\n",
    "\n",
    "# 处理特征\n",
    "def process_features(features):\n",
    "    # 判断，字典中新的key capital_indicator也同样对应一个tensor\n",
    "    capital_indicator = features['capital_gain'] > features['capital_loss']\n",
    "    features['capital_indicator'] = tf.cast(capital_indicator, dtype=tf.int32)\n",
    "    # 返回feature字典\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. 数据输入函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入到estimator的数据解析函数\n",
    "def csv_input_fn(file_names, mode=tf.estimator.ModeKeys.EVAL, \n",
    "                 skip_header_lines=0, \n",
    "                 num_epochs=None, \n",
    "                 batch_size=200):\n",
    "    \n",
    "    # 训练阶段数据要shuffle，测试阶段不用\n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    # 多线程\n",
    "    num_threads = multiprocessing.cpu_count() if MULTI_THREADING else 1\n",
    "    # 输出信息\n",
    "    print(\"\")\n",
    "    print(\"数据输入函数input_fn:\")\n",
    "    print(\"================\")\n",
    "    print(\"输入文件: {}\".format(file_names))\n",
    "    print(\"Batch size: {}\".format(batch_size))\n",
    "    print(\"Epoch Count: {}\".format(num_epochs))\n",
    "    print(\"模式: {}\".format(mode))\n",
    "    print(\"Thread Count: {}\".format(num_threads))\n",
    "    print(\"Shuffle: {}\".format(shuffle))\n",
    "    print(\"================\")\n",
    "    print(\"\")\n",
    "\n",
    "    \n",
    "    #file_names = tf.matching_files(files_name_pattern)\n",
    "    dataset = data.TextLineDataset(filenames=file_names)\n",
    "    # 跳过第一行\n",
    "    dataset = dataset.skip(skip_header_lines)\n",
    "    # 乱序\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "    # 取一个batch\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    # 对数据进行解析\n",
    "    dataset = dataset.map(lambda csv_row: parse_csv_row(csv_row), \n",
    "                          num_parallel_calls=num_threads)\n",
    "    # 如果做更多处理，添加新列\n",
    "    if PROCESS_FEATURES:\n",
    "        dataset = dataset.map(lambda features, target: (process_features(features), target), \n",
    "                              num_parallel_calls=num_threads)\n",
    "    # 每个epoch完成后，重启dataset  \n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    # 取出满足 特征字典+结果序列 的值\n",
    "    features, target = iterator.get_next()\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "数据输入函数input_fn:\n",
      "================\n",
      "输入文件: ['./adult_train.csv']\n",
      "Batch size: 200\n",
      "Epoch Count: None\n",
      "模式: eval\n",
      "Thread Count: 8\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "CSV文件的特征: ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'capital_indicator']\n",
      "CSV文件的标签: Tensor(\"IteratorGetNext:15\", shape=(?,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "features, target = csv_input_fn(file_names=[\"./adult_train.csv\"])\n",
    "print(\"CSV文件的特征: {}\".format(list(features.keys())))\n",
    "print(\"CSV文件的标签: {}\".format(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义特征列\n",
    "### a. 对数值型变量做幅度缩放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>stdv</th>\n",
       "      <th>feature_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>90</td>\n",
       "      <td>38.581647</td>\n",
       "      <td>17</td>\n",
       "      <td>13.640433</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_num</th>\n",
       "      <td>16</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>1</td>\n",
       "      <td>2.572720</td>\n",
       "      <td>education_num</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_gain</th>\n",
       "      <td>99999</td>\n",
       "      <td>1077.648844</td>\n",
       "      <td>0</td>\n",
       "      <td>7385.292085</td>\n",
       "      <td>capital_gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_loss</th>\n",
       "      <td>4356</td>\n",
       "      <td>87.303830</td>\n",
       "      <td>0</td>\n",
       "      <td>402.960219</td>\n",
       "      <td>capital_loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours_per_week</th>\n",
       "      <td>99</td>\n",
       "      <td>40.437456</td>\n",
       "      <td>1</td>\n",
       "      <td>12.347429</td>\n",
       "      <td>hours_per_week</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  max         mean  min         stdv    feature_name\n",
       "age                90    38.581647   17    13.640433             age\n",
       "education_num      16    10.080679    1     2.572720   education_num\n",
       "capital_gain    99999  1077.648844    0  7385.292085    capital_gain\n",
       "capital_loss     4356    87.303830    0   402.960219    capital_loss\n",
       "hours_per_week     99    40.437456    1    12.347429  hours_per_week"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats = pd.read_csv(\"./adult.stats.csv\", header=0, index_col=0)\n",
    "df_stats['feature_name'] = NUMERIC_FEATURE_NAMES\n",
    "df_stats.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. 构建不同的特征列(特征工程)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: {'age': _NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function get_feature_columns.<locals>.<lambda> at 0x10243c0d0>), 'education_num': _NumericColumn(key='education_num', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function get_feature_columns.<locals>.<lambda> at 0x10243c048>), 'capital_gain': _NumericColumn(key='capital_gain', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function get_feature_columns.<locals>.<lambda> at 0x10243cbf8>), 'capital_loss': _NumericColumn(key='capital_loss', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function get_feature_columns.<locals>.<lambda> at 0x10243c2f0>), 'hours_per_week': _NumericColumn(key='hours_per_week', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function get_feature_columns.<locals>.<lambda> at 0x10243c510>), 'gender': _VocabularyListCategoricalColumn(key='gender', vocabulary_list=('Female', 'Male'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'race': _VocabularyListCategoricalColumn(key='race', vocabulary_list=('Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'education': _VocabularyListCategoricalColumn(key='education', vocabulary_list=('Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college', 'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school', '5th-6th', '10th', '1st-4th', 'Preschool', '12th'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'marital_status': _VocabularyListCategoricalColumn(key='marital_status', vocabulary_list=('Married-civ-spouse', 'Divorced', 'Married-spouse-absent', 'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'relationship': _VocabularyListCategoricalColumn(key='relationship', vocabulary_list=('Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried', 'Other-relative'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'workclass': _VocabularyListCategoricalColumn(key='workclass', vocabulary_list=('Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov', 'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'), dtype=tf.string, default_value=-1, num_oov_buckets=0), 'capital_indicator': _IdentityCategoricalColumn(key='capital_indicator', num_buckets=2, default_value=0), 'occupation': _HashedCategoricalColumn(key='occupation', hash_bucket_size=50, dtype=tf.string), 'native_country': _HashedCategoricalColumn(key='native_country', hash_bucket_size=100, dtype=tf.string), 'age_buckets': _BucketizedColumn(source_column=_NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function get_feature_columns.<locals>.<lambda> at 0x10243c0d0>), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)), 'education_X_occupation': _CrossedColumn(keys=('education', 'occupation'), hash_bucket_size=10000, hash_key=None), 'age_buckets_X_race': _CrossedColumn(keys=(_BucketizedColumn(source_column=_NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function get_feature_columns.<locals>.<lambda> at 0x10243c0d0>), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)), _VocabularyListCategoricalColumn(key='race', vocabulary_list=('Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), hash_bucket_size=10000, hash_key=None), 'native_country_X_occupation': _CrossedColumn(keys=('native_country', 'occupation'), hash_bucket_size=10000, hash_key=None), 'native_country_embedded': _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='native_country', hash_bucket_size=100, dtype=tf.string), dimension=3, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x1023bc358>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), 'occupation_embedded': _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='occupation', hash_bucket_size=50, dtype=tf.string), dimension=3, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x1023bc390>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), 'education_X_occupation_embedded': _EmbeddingColumn(categorical_column=_CrossedColumn(keys=('education', 'occupation'), hash_bucket_size=10000, hash_key=None), dimension=3, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x1023bc3c8>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), 'native_country_X_occupation_embedded': _EmbeddingColumn(categorical_column=_CrossedColumn(keys=('native_country', 'occupation'), hash_bucket_size=10000, hash_key=None), dimension=3, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x1023bc400>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)}\n"
     ]
    }
   ],
   "source": [
    "# 使用tf构建的高级特征\n",
    "def extend_feature_columns(feature_columns, hparams):\n",
    "    \n",
    "    # 年龄分桶\n",
    "    age_buckets = tf.feature_column.bucketized_column(\n",
    "      feature_columns['age'], boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "    \n",
    "    # 特征交叉组合并hash分桶\n",
    "    education_X_occupation = tf.feature_column.crossed_column(\n",
    "     ['education', 'occupation'], hash_bucket_size=int(1e4))\n",
    "    \n",
    "    # 特征交叉组合并hash分桶\n",
    "    age_buckets_X_race = tf.feature_column.crossed_column(\n",
    "     [age_buckets, feature_columns['race']], hash_bucket_size=int(1e4))\n",
    "    \n",
    "    # 特征交叉组合并hash分桶\n",
    "    native_country_X_occupation = tf.feature_column.crossed_column(\n",
    "          ['native_country', 'occupation'], hash_bucket_size=int(1e4))\n",
    "    \n",
    "    # 对类别型特征做embedding\n",
    "    native_country_embedded = tf.feature_column.embedding_column(\n",
    "          feature_columns['native_country'], dimension=hparams['embedding_size'])\n",
    "    \n",
    "    # 对类别型特征做embedding\n",
    "    occupation_embedded = tf.feature_column.embedding_column(\n",
    "          feature_columns['occupation'], dimension=hparams['embedding_size'])\n",
    "    \n",
    "    # 同上\n",
    "    education_X_occupation_embedded = tf.feature_column.embedding_column(\n",
    "          education_X_occupation, dimension=hparams['embedding_size'])\n",
    "    \n",
    "    # 同上\n",
    "    native_country_X_occupation_embedded = tf.feature_column.embedding_column(\n",
    "          native_country_X_occupation, dimension=hparams['embedding_size'])\n",
    "    \n",
    "    # 构建feature columns\n",
    "    feature_columns['age_buckets'] = age_buckets\n",
    "    feature_columns['education_X_occupation'] = education_X_occupation\n",
    "    feature_columns['age_buckets_X_race'] = age_buckets_X_race\n",
    "    feature_columns['native_country_X_occupation'] = native_country_X_occupation\n",
    "    feature_columns['native_country_embedded'] = native_country_embedded\n",
    "    feature_columns['occupation_embedded'] = occupation_embedded\n",
    "    feature_columns['education_X_occupation_embedded'] = education_X_occupation_embedded\n",
    "    feature_columns['native_country_X_occupation_embedded'] = native_country_X_occupation_embedded\n",
    "    \n",
    "    # 返回feature_columns字典\n",
    "    return feature_columns\n",
    "\n",
    "# 标准化\n",
    "def standard_scaler(x, mean, stdv):\n",
    "    return (x-mean)/(stdv)\n",
    "\n",
    "# 最大最小值幅度缩放\n",
    "def maxmin_scaler(x, max_value, min_value):\n",
    "    return (x-min_value)/(max_value-min_value)  \n",
    "\n",
    "# 全部的特征\n",
    "def get_feature_columns(hparams):\n",
    "    \n",
    "    # 数值型的列\n",
    "    numeric_columns = {}\n",
    "    # 对数值型的列做幅度缩放(scaling)\n",
    "    for feature_name in NUMERIC_FEATURE_NAMES:\n",
    "\n",
    "        feature_mean = df_stats[df_stats.feature_name == feature_name]['mean'].values[0]\n",
    "        feature_stdv = df_stats[df_stats.feature_name == feature_name]['stdv'].values[0]\n",
    "        normalizer_fn = lambda x: standard_scaler(x, feature_mean, feature_stdv)\n",
    "        \n",
    "        numeric_columns[feature_name] = tf.feature_column.numeric_column(feature_name, \n",
    "                                                                         normalizer_fn=normalizer_fn)\n",
    "    # 新构建列(这里没有)                                                                  \n",
    "    CONSTRUCTED_NUMERIC_FEATURES_NAMES = []\n",
    "    \n",
    "    if PROCESS_FEATURES:\n",
    "        for feature_name in CONSTRUCTED_NUMERIC_FEATURES_NAMES:\n",
    "            numeric_columns[feature_name] = tf.feature_column.numeric_column(feature_name)\n",
    "    \n",
    "    # 对类别型的列做独热向量编码\n",
    "    categorical_column_with_vocabulary = \\\n",
    "        {item[0]: tf.feature_column.categorical_column_with_vocabulary_list(item[0], item[1])\n",
    "         for item in CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.items()}\n",
    "    \n",
    "    # indicator列，multi-hot编码\n",
    "    CONSTRUCTED_INDICATOR_FEATURES_NAMES = ['capital_indicator']\n",
    "    \n",
    "    categorical_column_with_identity = {}\n",
    "    \n",
    "    for feature_name in CONSTRUCTED_INDICATOR_FEATURES_NAMES: \n",
    "        categorical_column_with_identity[feature_name] = tf.feature_column.categorical_column_with_identity(feature_name, \n",
    "                                                                                                              num_buckets=2,\n",
    "                                                                                                            default_value=0)\n",
    "    # 类别型进行hash分桶映射                                                                                                          \n",
    "    categorical_column_with_hash_bucket = \\\n",
    "        {item[0]: tf.feature_column.categorical_column_with_hash_bucket(item[0], item[1], dtype=tf.string)\n",
    "         for item in CATEGORICAL_FEATURE_NAMES_WITH_BUCKET_SIZE.items()}\n",
    "        \n",
    "    feature_columns = {}\n",
    "\n",
    "    # 更新数值列                                                                                                        \n",
    "    if numeric_columns is not None:\n",
    "        feature_columns.update(numeric_columns)\n",
    "\n",
    "    # 更新独热向量编码列\n",
    "    if categorical_column_with_vocabulary is not None:\n",
    "        feature_columns.update(categorical_column_with_vocabulary)\n",
    "    \n",
    "    # 更新label encoder列\n",
    "    if categorical_column_with_identity is not None:\n",
    "        feature_columns.update(categorical_column_with_identity)\n",
    "                                                                                                            \n",
    "    # 更新类别型hash分桶列    \n",
    "    if categorical_column_with_hash_bucket is not None:\n",
    "        feature_columns.update(categorical_column_with_hash_bucket)\n",
    "                                                                                                            \n",
    "    # 扩充tf产出的高级列\n",
    "    if EXTEND_FEATURE_COLUMNS:\n",
    "        feature_columns = extend_feature_columns(feature_columns, hparams)\n",
    "                                                                                                            \n",
    "    # 返回feature columns   \n",
    "    return feature_columns\n",
    "\n",
    "feature_columns = get_feature_columns(hparams={\"num_buckets\":5,\"embedding_size\":3})\n",
    "print(\"Feature Columns: {}\".format(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义一个DNN  Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. 获取宽度和深度的特征列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型当中需要的宽度和深度特征\n",
    "def get_wide_deep_columns():\n",
    "    # 所有列名\n",
    "    feature_columns = list(get_feature_columns(hparams).values())\n",
    "    # 过滤出深度部分的特征\n",
    "    dense_columns = list(\n",
    "        filter(lambda column: isinstance(column, feature_column._NumericColumn) |\n",
    "                              isinstance(column, feature_column._EmbeddingColumn),\n",
    "               feature_columns\n",
    "        )\n",
    "    )\n",
    "    # 过滤出类别型的特征\n",
    "    categorical_columns = list(\n",
    "        filter(lambda column: isinstance(column, feature_column._VocabularyListCategoricalColumn) |\n",
    "                              isinstance(column, feature_column._IdentityCategoricalColumn) |\n",
    "                              isinstance(column, feature_column._BucketizedColumn),\n",
    "                   feature_columns)\n",
    "    )\n",
    "    # 稀疏特征(也是在wide部分的)\n",
    "    sparse_columns = list(\n",
    "        filter(lambda column: isinstance(column,feature_column._HashedCategoricalColumn) |\n",
    "                              isinstance(column, feature_column._CrossedColumn),\n",
    "               feature_columns)\n",
    "    )\n",
    "    # 指示列特征\n",
    "    indicator_columns = list(\n",
    "            map(lambda column: tf.feature_column.indicator_column(column),\n",
    "                categorical_columns)\n",
    "    )\n",
    "    # 明确deep和wide部分需要的特征列\n",
    "    deep_feature_columns = dense_columns + indicator_columns\n",
    "    wide_feature_columns = categorical_columns + sparse_columns\n",
    "    \n",
    "    # 返回deep和wide部分的特征列\n",
    "    return wide_feature_columns, deep_feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. 定义estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DNNComb_estimator(run_config, hparams, print_desc=False):\n",
    "    \n",
    "    # 取到返回的特征列\n",
    "    wide_feature_columns, deep_feature_columns = get_wide_deep_columns()\n",
    "    \n",
    "    # 构建宽度深度模型\n",
    "    estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "        \n",
    "        # 指定分类类别的个数\n",
    "        n_classes=len(TARGET_LABELS),\n",
    "        # 如果类别不是从0到n-1的n个连续整数，则需要指定不同类别(用一个list)\n",
    "        label_vocabulary=TARGET_LABELS,\n",
    "        \n",
    "        # 定义宽度和深度列\n",
    "        dnn_feature_columns = deep_feature_columns,\n",
    "        linear_feature_columns = wide_feature_columns,\n",
    "        \n",
    "        # 定义样本权重列\n",
    "        weight_column=WEIGHT_COLUMN_NAME,\n",
    "        \n",
    "        # 关于DNN隐层的一些设定\n",
    "        dnn_hidden_units= hparams[\"hidden_units\"],\n",
    "        # 优化器的选择\n",
    "        dnn_optimizer= tf.train.AdamOptimizer(),\n",
    "        # 激活函数的选择\n",
    "        dnn_activation_fn= tf.nn.relu,\n",
    "        \n",
    "        # 配置\n",
    "        config= run_config\n",
    "    )\n",
    "    \n",
    "    \n",
    "    if print_desc:\n",
    "        print(\"\")\n",
    "        print(\"预估器类型:\")\n",
    "        print(\"================\")\n",
    "        print(type(estimator))\n",
    "        print(\"\")\n",
    "        print(\"深度部分的列名:\")\n",
    "        print(\"==============\")\n",
    "        print(deep_feature_columns)\n",
    "        print(\"\")\n",
    "        print(\"宽度部分的列名:\")\n",
    "        print(\"=============\")\n",
    "        print(wide_feature_columns)\n",
    "        print(\"\")\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 构建实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. 设定参数与运行参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epochs': 100, 'batch_size': 500, 'embedding_size': 4, 'hidden_units': [64, 32, 16], 'max_steps': 6512.2}\n",
      "模型目录: trained_models/cenus-model-01\n",
      "\n",
      "数据集大小: 32561\n",
      "Batch大小: 500\n",
      "每个Epoch的迭代次数: 65.122\n",
      "总迭代次数: 6512.2\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = TRAIN_DATA_SIZE\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 500\n",
    "EVAL_AFTER_SEC = 60\n",
    "TOTAL_STEPS = (TRAIN_SIZE/BATCH_SIZE)*NUM_EPOCHS\n",
    "\n",
    "hparams  = {\n",
    "    \"num_epochs\" : NUM_EPOCHS,\n",
    "    \"batch_size\" : BATCH_SIZE,\n",
    "    \"embedding_size\" : 4,\n",
    "    \"hidden_units\" : [64, 32, 16],\n",
    "    \"max_steps\" : TOTAL_STEPS}\n",
    "\n",
    "model_dir = 'trained_models/{}'.format(MODEL_NAME)\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    log_step_count_steps=5000,\n",
    "    tf_random_seed=201805,\n",
    "    model_dir=model_dir\n",
    ")\n",
    "\n",
    "print(hparams)\n",
    "print(\"模型目录:\", run_config.model_dir)\n",
    "print(\"\")\n",
    "print(\"数据集大小:\", TRAIN_SIZE)\n",
    "print(\"Batch大小:\", BATCH_SIZE)\n",
    "print(\"每个Epoch的迭代次数:\",TRAIN_SIZE/BATCH_SIZE)\n",
    "print(\"总迭代次数:\", TOTAL_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. 定义train_and_eval 需要的配置TrainSpec和EvaluSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(\n",
    "    # 给的第一个是input_fn\n",
    "    input_fn = lambda: csv_input_fn(\n",
    "        TRAIN_DATA_FILES_PATTERN,\n",
    "        mode = tf.estimator.ModeKeys.TRAIN,\n",
    "        num_epochs=hparams[\"num_epochs\"],\n",
    "        batch_size=hparams[\"batch_size\"]\n",
    "    ),\n",
    "    # 最大迭代次数\n",
    "    max_steps=hparams[\"max_steps\"],\n",
    "    # 可以附加一些其他功能\n",
    "    hooks=None\n",
    ")\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn = lambda: csv_input_fn(\n",
    "        TRAIN_DATA_FILES_PATTERN,\n",
    "        mode=tf.estimator.ModeKeys.EVAL,\n",
    "        num_epochs=1,\n",
    "        batch_size=hparams[\"batch_size\"],\n",
    "            \n",
    "    ),\n",
    "    throttle_secs = EVAL_AFTER_SEC,\n",
    "    steps=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. 通过train_and_evaluate跑实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清除之前训练的结果...\n",
      "训练与验证开始于10:45:58\n",
      ".......................................\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'trained_models/cenus-model-01', '_tf_random_seed': 201805, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 5000, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1023d7400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "预估器类型:\n",
      "================\n",
      "<class 'tensorflow.python.estimator.canned.dnn_linear_combined.DNNLinearCombinedClassifier'>\n",
      "\n",
      "深度部分的列名:\n",
      "==============\n",
      "[_NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function get_feature_columns.<locals>.<lambda> at 0x1023ba6a8>), _NumericColumn(key='education_num', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function get_feature_columns.<locals>.<lambda> at 0x1023ba7b8>), _NumericColumn(key='capital_gain', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function get_feature_columns.<locals>.<lambda> at 0x1023ba730>), _NumericColumn(key='capital_loss', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function get_feature_columns.<locals>.<lambda> at 0x1023ba840>), _NumericColumn(key='hours_per_week', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function get_feature_columns.<locals>.<lambda> at 0x1023ba950>), _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='native_country', hash_bucket_size=100, dtype=tf.string), dimension=4, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x1023d4438>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='occupation', hash_bucket_size=50, dtype=tf.string), dimension=4, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x1023d4b00>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), _EmbeddingColumn(categorical_column=_CrossedColumn(keys=('education', 'occupation'), hash_bucket_size=10000, hash_key=None), dimension=4, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x1023d4828>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), _EmbeddingColumn(categorical_column=_CrossedColumn(keys=('native_country', 'occupation'), hash_bucket_size=10000, hash_key=None), dimension=4, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x1023d4e48>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True), _IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='gender', vocabulary_list=('Female', 'Male'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), _IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='race', vocabulary_list=('Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), _IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='education', vocabulary_list=('Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college', 'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school', '5th-6th', '10th', '1st-4th', 'Preschool', '12th'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), _IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='marital_status', vocabulary_list=('Married-civ-spouse', 'Divorced', 'Married-spouse-absent', 'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), _IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='relationship', vocabulary_list=('Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried', 'Other-relative'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), _IndicatorColumn(categorical_column=_VocabularyListCategoricalColumn(key='workclass', vocabulary_list=('Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov', 'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), _IndicatorColumn(categorical_column=_IdentityCategoricalColumn(key='capital_indicator', num_buckets=2, default_value=0)), _IndicatorColumn(categorical_column=_BucketizedColumn(source_column=_NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function get_feature_columns.<locals>.<lambda> at 0x1023ba6a8>), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)))]\n",
      "\n",
      "宽度部分的列名:\n",
      "=============\n",
      "[_VocabularyListCategoricalColumn(key='gender', vocabulary_list=('Female', 'Male'), dtype=tf.string, default_value=-1, num_oov_buckets=0), _VocabularyListCategoricalColumn(key='race', vocabulary_list=('Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'), dtype=tf.string, default_value=-1, num_oov_buckets=0), _VocabularyListCategoricalColumn(key='education', vocabulary_list=('Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college', 'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school', '5th-6th', '10th', '1st-4th', 'Preschool', '12th'), dtype=tf.string, default_value=-1, num_oov_buckets=0), _VocabularyListCategoricalColumn(key='marital_status', vocabulary_list=('Married-civ-spouse', 'Divorced', 'Married-spouse-absent', 'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'), dtype=tf.string, default_value=-1, num_oov_buckets=0), _VocabularyListCategoricalColumn(key='relationship', vocabulary_list=('Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried', 'Other-relative'), dtype=tf.string, default_value=-1, num_oov_buckets=0), _VocabularyListCategoricalColumn(key='workclass', vocabulary_list=('Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov', 'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'), dtype=tf.string, default_value=-1, num_oov_buckets=0), _IdentityCategoricalColumn(key='capital_indicator', num_buckets=2, default_value=0), _BucketizedColumn(source_column=_NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function get_feature_columns.<locals>.<lambda> at 0x1023ba6a8>), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)), _HashedCategoricalColumn(key='occupation', hash_bucket_size=50, dtype=tf.string), _HashedCategoricalColumn(key='native_country', hash_bucket_size=100, dtype=tf.string), _CrossedColumn(keys=('education', 'occupation'), hash_bucket_size=10000, hash_key=None), _CrossedColumn(keys=(_BucketizedColumn(source_column=_NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function get_feature_columns.<locals>.<lambda> at 0x1023ba6a8>), boundaries=(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)), _VocabularyListCategoricalColumn(key='race', vocabulary_list=('Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), hash_bucket_size=10000, hash_key=None), _CrossedColumn(keys=('native_country', 'occupation'), hash_bucket_size=10000, hash_key=None)]\n",
      "\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 60 secs (eval_spec.throttle_secs) or training is finished.\n",
      "\n",
      "数据输入函数input_fn:\n",
      "================\n",
      "输入文件: adult_train.csv\n",
      "Batch size: 500\n",
      "Epoch Count: 100\n",
      "模式: train\n",
      "Thread Count: 8\n",
      "Shuffle: True\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into trained_models/cenus-model-01/model.ckpt.\n",
      "INFO:tensorflow:loss = 221356980.0, step = 1\n",
      "INFO:tensorflow:loss = 37123250.0, step = 101 (1.553 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 27724134.0, step = 201 (0.505 sec)\n",
      "INFO:tensorflow:loss = 26221192.0, step = 301 (0.500 sec)\n",
      "INFO:tensorflow:loss = 30562312.0, step = 401 (0.500 sec)\n",
      "INFO:tensorflow:loss = 26799344.0, step = 501 (0.493 sec)\n",
      "INFO:tensorflow:loss = 28504848.0, step = 601 (0.489 sec)\n",
      "INFO:tensorflow:loss = 31572938.0, step = 701 (0.495 sec)\n",
      "INFO:tensorflow:loss = 34452524.0, step = 801 (0.493 sec)\n",
      "INFO:tensorflow:loss = 24246322.0, step = 901 (0.518 sec)\n",
      "INFO:tensorflow:loss = 24602954.0, step = 1001 (0.562 sec)\n",
      "INFO:tensorflow:loss = 29845566.0, step = 1101 (0.509 sec)\n",
      "INFO:tensorflow:loss = 26672250.0, step = 1201 (0.510 sec)\n",
      "INFO:tensorflow:loss = 27849432.0, step = 1301 (0.512 sec)\n",
      "INFO:tensorflow:loss = 27291776.0, step = 1401 (0.527 sec)\n",
      "INFO:tensorflow:loss = 24043900.0, step = 1501 (0.555 sec)\n",
      "INFO:tensorflow:loss = 30703954.0, step = 1601 (0.553 sec)\n",
      "INFO:tensorflow:loss = 27262016.0, step = 1701 (0.515 sec)\n",
      "INFO:tensorflow:loss = 26068582.0, step = 1801 (0.516 sec)\n",
      "INFO:tensorflow:loss = 23579862.0, step = 1901 (0.491 sec)\n",
      "INFO:tensorflow:loss = 27232648.0, step = 2001 (0.503 sec)\n",
      "INFO:tensorflow:loss = 29843886.0, step = 2101 (0.500 sec)\n",
      "INFO:tensorflow:loss = 29093932.0, step = 2201 (0.503 sec)\n",
      "INFO:tensorflow:loss = 26308662.0, step = 2301 (0.490 sec)\n",
      "INFO:tensorflow:loss = 28543626.0, step = 2401 (0.537 sec)\n",
      "INFO:tensorflow:loss = 22722266.0, step = 2501 (0.492 sec)\n",
      "INFO:tensorflow:loss = 25338066.0, step = 2601 (0.532 sec)\n",
      "INFO:tensorflow:loss = 22729360.0, step = 2701 (0.496 sec)\n",
      "INFO:tensorflow:loss = 26087186.0, step = 2801 (0.516 sec)\n",
      "INFO:tensorflow:loss = 24704950.0, step = 2901 (0.486 sec)\n",
      "INFO:tensorflow:loss = 24299124.0, step = 3001 (0.518 sec)\n",
      "INFO:tensorflow:loss = 25369730.0, step = 3101 (0.472 sec)\n",
      "INFO:tensorflow:loss = 29020614.0, step = 3201 (0.517 sec)\n",
      "INFO:tensorflow:loss = 24960058.0, step = 3301 (0.508 sec)\n",
      "INFO:tensorflow:loss = 23668320.0, step = 3401 (0.509 sec)\n",
      "INFO:tensorflow:loss = 24328068.0, step = 3501 (0.497 sec)\n",
      "INFO:tensorflow:loss = 20087708.0, step = 3601 (0.500 sec)\n",
      "INFO:tensorflow:loss = 22424498.0, step = 3701 (0.489 sec)\n",
      "INFO:tensorflow:loss = 25188584.0, step = 3801 (0.505 sec)\n",
      "INFO:tensorflow:loss = 30233150.0, step = 3901 (0.498 sec)\n",
      "INFO:tensorflow:loss = 30482144.0, step = 4001 (0.502 sec)\n",
      "INFO:tensorflow:loss = 31028692.0, step = 4101 (0.495 sec)\n",
      "INFO:tensorflow:loss = 21967958.0, step = 4201 (0.497 sec)\n",
      "INFO:tensorflow:loss = 25391734.0, step = 4301 (0.495 sec)\n",
      "INFO:tensorflow:loss = 19357908.0, step = 4401 (0.492 sec)\n",
      "INFO:tensorflow:loss = 23701582.0, step = 4501 (0.496 sec)\n",
      "INFO:tensorflow:loss = 19444038.0, step = 4601 (0.505 sec)\n",
      "INFO:tensorflow:loss = 22775396.0, step = 4701 (0.493 sec)\n",
      "INFO:tensorflow:loss = 23660576.0, step = 4801 (0.556 sec)\n",
      "INFO:tensorflow:loss = 24774512.0, step = 4901 (0.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.22\n",
      "INFO:tensorflow:loss = 22288336.0, step = 5001 (0.521 sec)\n",
      "INFO:tensorflow:loss = 23933690.0, step = 5101 (0.517 sec)\n",
      "INFO:tensorflow:loss = 23902232.0, step = 5201 (0.528 sec)\n",
      "INFO:tensorflow:loss = 24164714.0, step = 5301 (0.524 sec)\n",
      "INFO:tensorflow:loss = 25362028.0, step = 5401 (0.532 sec)\n",
      "INFO:tensorflow:loss = 22645134.0, step = 5501 (0.511 sec)\n",
      "INFO:tensorflow:loss = 23062932.0, step = 5601 (0.493 sec)\n",
      "INFO:tensorflow:loss = 23826188.0, step = 5701 (0.512 sec)\n",
      "INFO:tensorflow:loss = 21558810.0, step = 5801 (0.495 sec)\n",
      "INFO:tensorflow:loss = 23853958.0, step = 5901 (0.521 sec)\n",
      "INFO:tensorflow:loss = 25435616.0, step = 6001 (0.502 sec)\n",
      "INFO:tensorflow:loss = 23588904.0, step = 6101 (0.535 sec)\n",
      "INFO:tensorflow:loss = 23318888.0, step = 6201 (0.483 sec)\n",
      "INFO:tensorflow:loss = 22294292.0, step = 6301 (0.517 sec)\n",
      "INFO:tensorflow:loss = 22270804.0, step = 6401 (0.482 sec)\n",
      "INFO:tensorflow:loss = 20967080.0, step = 6501 (0.536 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6513 into trained_models/cenus-model-01/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 20564190.0.\n",
      "\n",
      "数据输入函数input_fn:\n",
      "================\n",
      "输入文件: adult_train.csv\n",
      "Batch size: 500\n",
      "Epoch Count: 1\n",
      "模式: eval\n",
      "Thread Count: 8\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-18-02:46:41\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cenus-model-01/model.ckpt-6513\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-18-02:46:43\n",
      "INFO:tensorflow:Saving dict for global step 6513: accuracy = 0.892855, accuracy_baseline = 0.7614407, auc = 0.95204127, auc_precision_recall = 0.86319804, average_loss = 0.23794046, global_step = 6513, label/mean = 0.2385593, loss = 22277624.0, prediction/mean = 0.252381\n",
      ".......................................\n",
      "训练与验证结束于10:46:44\n",
      "\n",
      "训练和验证实验总耗时46.012707秒\n"
     ]
    }
   ],
   "source": [
    "if not RESUME_TRAINING:\n",
    "    print(\"清除之前训练的结果...\")\n",
    "    shutil.rmtree(model_dir, ignore_errors=True)\n",
    "else:\n",
    "    print(\"从上一次训练重新加载继续训练...\") \n",
    "\n",
    "    \n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "time_start = datetime.now() \n",
    "print(\"训练与验证开始于{}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "print(\".......................................\") \n",
    "\n",
    "estimator = create_DNNComb_estimator(run_config, hparams, True)\n",
    "\n",
    "tf.estimator.train_and_evaluate(\n",
    "    estimator=estimator,\n",
    "    train_spec=train_spec, \n",
    "    eval_spec=eval_spec\n",
    ")\n",
    "\n",
    "time_end = datetime.now() \n",
    "print(\".......................................\")\n",
    "print(\"训练与验证结束于{}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "print(\"\")\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"训练和验证实验总耗时{}秒\".format(time_elapsed.total_seconds()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'trained_models/cenus-model-01', '_tf_random_seed': 201805, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 5000, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1023d7400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "数据输入函数input_fn:\n",
      "================\n",
      "输入文件: adult_train.csv\n",
      "Batch size: 32561\n",
      "Epoch Count: None\n",
      "模式: eval\n",
      "Thread Count: 8\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-18-02:46:46\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cenus-model-01/model.ckpt-6513\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-18-02:46:48\n",
      "INFO:tensorflow:Saving dict for global step 6513: accuracy = 0.8928556, accuracy_baseline = 0.76144063, auc = 0.95204127, auc_precision_recall = 0.863198, average_loss = 0.23794033, global_step = 6513, label/mean = 0.23855937, loss = 1470321900.0, prediction/mean = 0.25238076\n",
      "\n",
      "######################################################################################\n",
      "# 训练结果指标: {'accuracy': 0.8928556, 'accuracy_baseline': 0.76144063, 'auc': 0.95204127, 'auc_precision_recall': 0.863198, 'average_loss': 0.23794033, 'label/mean': 0.23855937, 'loss': 1470321900.0, 'prediction/mean': 0.25238076, 'global_step': 6513}\n",
      "######################################################################################\n",
      "\n",
      "数据输入函数input_fn:\n",
      "================\n",
      "输入文件: adult_test.csv\n",
      "Batch size: 16279\n",
      "Epoch Count: None\n",
      "模式: eval\n",
      "Thread Count: 8\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-18-02:46:50\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cenus-model-01/model.ckpt-6513\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-18-02:46:51\n",
      "INFO:tensorflow:Saving dict for global step 6513: accuracy = 0.84456533, accuracy_baseline = 0.7638324, auc = 0.89273417, auc_precision_recall = 0.71950656, average_loss = 0.37453595, global_step = 6513, label/mean = 0.23616762, loss = 1155045400.0, prediction/mean = 0.24662538\n",
      "\n",
      "######################################################################################\n",
      "# 测试结果指标: {'accuracy': 0.84456533, 'accuracy_baseline': 0.7638324, 'auc': 0.89273417, 'auc_precision_recall': 0.71950656, 'average_loss': 0.37453595, 'label/mean': 0.23616762, 'loss': 1155045400.0, 'prediction/mean': 0.24662538, 'global_step': 6513}\n",
      "######################################################################################\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = TRAIN_DATA_SIZE\n",
    "TEST_SIZE = TEST_DATA_SIZE\n",
    "\n",
    "train_input_fn = lambda: csv_input_fn(file_names= TRAIN_DATA_FILES_PATTERN, \n",
    "                                      mode= tf.estimator.ModeKeys.EVAL,\n",
    "                                      batch_size= TRAIN_SIZE)\n",
    "\n",
    "test_input_fn = lambda: csv_input_fn(file_names= TEST_DATA_FILES_PATTERN, \n",
    "                                      mode= tf.estimator.ModeKeys.EVAL,\n",
    "                                      batch_size= TEST_SIZE)\n",
    "\n",
    "estimator = create_DNNComb_estimator(run_config, hparams)\n",
    "\n",
    "train_results = estimator.evaluate(input_fn=train_input_fn, steps=1)\n",
    "print()\n",
    "print(\"######################################################################################\")\n",
    "print(\"# 训练结果指标: {}\".format(train_results))\n",
    "print(\"######################################################################################\")\n",
    "\n",
    "test_results = estimator.evaluate(input_fn=test_input_fn, steps=1)\n",
    "print()\n",
    "print(\"######################################################################################\")\n",
    "print(\"# 测试结果指标: {}\".format(test_results))\n",
    "print(\"######################################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "数据输入函数input_fn:\n",
      "================\n",
      "输入文件: adult_test.csv\n",
      "Batch size: 10\n",
      "Epoch Count: None\n",
      "模式: infer\n",
      "Thread Count: 8\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cenus-model-01/model.ckpt-6513\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "\n",
      "预测的类别: [0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "预测概率为: [[0.99999297, 6.992759e-06], [0.9547619, 0.045238085], [0.86959815, 0.13040185], [0.21073815, 0.7892619], [0.9999995, 5.3270975e-07], [0.99999905, 9.3406624e-07], [0.99990857, 9.1373935e-05], [0.42117104, 0.57882893], [0.9976412, 0.0023588594], [0.9993327, 0.00066724705]]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# 输入\n",
    "predict_input_fn = lambda: csv_input_fn(TEST_DATA_FILES_PATTERN, \n",
    "                                      mode= tf.estimator.ModeKeys.PREDICT,\n",
    "                                      batch_size= 10)\n",
    "\n",
    "\n",
    "predictions = list(itertools.islice(estimator.predict(input_fn=predict_input_fn),10))\n",
    "\n",
    "print(\"\")\n",
    "print(\"预测的类别: {}\".format(list(map(lambda item: item[\"class_ids\"][0]\n",
    "    ,predictions))))\n",
    "\n",
    "print(\"预测概率为: {}\".format(list(map(lambda item: list(item[\"probabilities\"])\n",
    "    ,predictions))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
