{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 基于tensorflow的深度学习\n",
    "\n",
    "## 0.背景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google开源了tensorflow这个并不是太好写的工具库给大家用，随后资源开始大幅度倾斜，不管是研究界还是工业界(不得不说，tensorflow对于分布式的支持还是非常棒的)，大量的代码和项目都是由tensorflow完成的。\n",
    "\n",
    "而在大家疯狂吐槽tensorflow“难写”之后，google也迅速做了一些调整，开放了高级接口，在不影响效率的情况下，让大家可以方便地处理数据、完成特征工程、构建各种模型、进行结果评估。\n",
    "![](https://3.bp.blogspot.com/-l2UT45WGdyw/Wbe7au1nfwI/AAAAAAAAD1I/GeQcQUUWezIiaFFRCiMILlX2EYdG49C0wCLcBGAs/s1600/image6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今天的课程里，我们带大家来认识一下tensorflow，认识一下从1.0之后版本开始快速成长起来的易用高层API接口。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 1.3 之后引入了两个重要功能：\n",
    "* **数据集：**一种创建输入管道（即，将数据读入您的程序）的全新方式。\n",
    "* **估算器（评估器）：**一种快速创建 TensorFlow 模型的高层API接口。TensorFlow框架提供了很多实用的预定制的估算器，如DNN、Boost tree等，同时框架还支持用户按照要求根据实际业务自定义评估器。\n",
    "\n",
    "同时，tf.feature_column中提供了非常实用的特征构造和预处理的函数。\n",
    "\n",
    "下面是它们在 TensorFlow 架构内的装配方式。结合使用数据集和估算器，可以轻松地用 TensorFlow 构建各种模型和做数据处理以及向模型提供数据："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.示例场景与模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以一个简单的场景为例，给大家讲解一下数据集和估算器的使用方式。\n",
    "\n",
    "数据集是大家熟悉的iris，模型的任务是经过训练，可以根据花的4个特征（萼片长度、萼片宽度、花瓣长度和花瓣宽度）对鸢尾花进行分类。\n",
    "\n",
    "![](https://www.tensorflow.org/images/iris_three_species.jpg)\n",
    "<center>从左到右依次为：山鸢尾、变色鸢尾和维吉尼亚鸢尾</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里用到的神经网络是最简单和直接的MLP(多层感知器)，所有输入和输出值都是 float32类型的，输出值的总和将等于 1（因为尾部是一个softmax）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://1.bp.blogspot.com/-EEdRK1mK1QQ/Wbe7qPWECZI/AAAAAAAAD1Q/fjnpGIiRIosTZ3YupkgiKJVaBtPg8KvGwCLcBGAs/s1600/image3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，输出结果对山鸢尾来说可能是 0.05，对变色鸢尾是 0.9，对维吉尼亚鸢尾是 0.05，表示这种花有 90% 的可能性是变色鸢尾。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Tensorflow数据集(Dataset)\n",
    "数据集是一种为 TensorFlow 模型创建输入管道的新方式。使用此 API 的性能要比使用 feed_dict 或队列式管道的性能高得多，而且此 API 更简洁，使用起来更容易。\n",
    "\n",
    "从高层次而言，数据集由以下类组成：\n",
    "![](https://www.tensorflow.org/images/dataset_classes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中：\n",
    "* 数据集/Dataset：基类，包含用于创建和转换数据集的函数。允许您从内存中的数据或从 Python 生成器初始化数据集。\n",
    "* TextLineDataset：从文本文件中读取各行内容。\n",
    "* TFRecordDataset：从 TFRecord 文件中读取记录。\n",
    "* FixedLengthRecordDataset：从二进制文件中读取固定大小的记录。\n",
    "* 迭代器：提供了一种一次获取一个数据集元素的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 本问题中的数据集\n",
    "首先，我们来看一下要用来为模型提供数据的数据集。我们将从一个 CSV 文件读取数据，这个文件的每一行都包含五个值 - 四个输入值，加上标签：\n",
    "![](https://2.bp.blogspot.com/-_MO8qp5MC40/Wbfw__Cw9pI/AAAAAAAAD1w/iOyE8ehYCzAbeeQQ9-Ik0IiVEA_P91VsACLcBGAs/s1600/image5.jpg)\n",
    "结果标签的值如下所述：\n",
    "* 山鸢尾为 0\n",
    "* 变色鸢尾为 1\n",
    "* 维吉尼亚鸢尾为 2。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.2 表示我们的数据集\n",
    "为了说明我们的数据集，我们先来创建一个特征列表：\n",
    "```python\n",
    "feature_names = [\n",
    "    'SepalLength',\n",
    "    'SepalWidth',\n",
    "    'PetalLength',\n",
    "    'PetalWidth']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练模型时，我们需要一个可以读取输入文件并返回特征和标签数据的函数。估算器要求您创建一个具有以下格式的函数：\n",
    "```python\n",
    "def input_fn():\n",
    "    ...<code>...\n",
    "    return ({ 'SepalLength':[values], ..<etc>.., 'PetalWidth':[values] },\n",
    "            [IrisFlowerType])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "返回值必须是一个按照如下方式组织的两元素元组：\n",
    "* 第一个元素必须是一个字典（其中的每个输入特征都是一个键），然后是一个用于训练批次的值列表。\n",
    "* 第二个元素是一个用于训练批次的标签列表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们要返回一批(一个batch)输入特征和训练标签，返回语句中的所有列表都将具有相同的长度。从技术角度而言，我们在这里说的“列表”实际上是指 1-d TensorFlow 张量。\n",
    "为了方便重复使用 **input_fn**，我们将向其中添加一些参数。这样，我们就可以使用不同设置构建输入函数。参数非常直观：\n",
    "* **file_path**：要读取的数据文件。\n",
    "* **perform_shuffle**：是否应将记录顺序随机化。\n",
    "* **repeat_count**：在数据集中迭代记录的次数。例如，如果我们指定 1，那么每个记录都将读取一次。如果我们不指定，迭代将永远持续下去。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是我们使用 Dataset API 实现此函数的方式。我们会将它包装到一个“输入函数”中，这个输入函数稍后将用于为我们的估算器模型提供数据："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "def my_input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
    "   def decode_csv(line):\n",
    "       parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])\n",
    "       # 最后一个元素是label\n",
    "       label = parsed_line[-1]\n",
    "       # 删除最后一个元素\n",
    "       del parsed_line[-1] \n",
    "       # 除掉最后一个元素其他都是特征\n",
    "       features = parsed_line \n",
    "       d = dict(zip(feature_names, features)), label\n",
    "       return d\n",
    "    \n",
    "   # 读取文本文件\n",
    "   dataset = (tf.data.TextLineDataset(file_path)\n",
    "       .skip(1) # 跳过csv头一行\n",
    "       .map(decode_csv)) # 通过decode_csv函数对每一行解析变换\n",
    "   if perform_shuffle:\n",
    "       # 使用大小为256的窗口对数据进行乱序\n",
    "       dataset = dataset.shuffle(buffer_size=256)\n",
    "   dataset = dataset.repeat(repeat_count)\n",
    "   # 设定batch size\n",
    "   dataset = dataset.batch(32)\n",
    "   # 构建iterator\n",
    "   iterator = dataset.make_one_shot_iterator()\n",
    "   batch_features, batch_labels = iterator.get_next()\n",
    "   return batch_features, batch_labels\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特别注意代码里的一些内容：\n",
    "* **TextLineDataset**：在您使用 Dataset API 的文件式数据集时，它将为您执行大量的内存管理工作。例如，您可以读入比内存大得多的数据集文件，或者以参数形式指定列表，读入多个文件。\n",
    "* **shuffle**：读取 buffer_size 记录，然后打乱（随机化）它们的顺序。\n",
    "* **map**：调用 **decode_csv** 函数，并将数据集中的每个元素作为一个参数（由于我们使用的是 TextLineDataset，每个元素都将是一行 CSV 文本）。然后，我们将向每一行应用 **decode_csv** 。\n",
    "* **decode_csv**：将每一行拆分成各个字段，根据需要提供默认值。然后，返回一个包含字段键和字段值的字典。map 函数将使用字典更新数据集中的每个元素（行）。\n",
    "* **prefetch()**：预取，input piprline performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上是数据集的简单介绍！简单跑一下，我们使用下面的函数打印第一个批次："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "next_batch = my_input_fn(FILE, True) # Will return 32 random elements\n",
    "\n",
    "# 在tf session中跑一下\n",
    "with tf.Session() as sess:\n",
    "    first_batch = sess.run(next_batch)\n",
    "print(first_batch)\n",
    "\n",
    "# 输出\n",
    "({'SepalLength': array([ 5.4000001, ...<repeat to 32 elems>], dtype=float32),\n",
    "  'PetalWidth': array([ 0.40000001, ...<repeat to 32 elems>], dtype=float32),\n",
    "  ...\n",
    " },\n",
    " [array([[2], ...<repeat to 32 elems>], dtype=int32) # Labels\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 估算器介绍\n",
    "估算器是一种高级 API，使用这种 API，您在训练 TensorFlow 模型时就不再像之前那样需要编写大量的样板文件代码。估算器也非常灵活，如果您对模型有具体的要求，它允许您替换默认行为。\n",
    "\n",
    "使用估算器，您可以通过两种可能的方式构建模型：\n",
    "\n",
    "* 预制估算器 - 这些是预先定义的估算器，旨在生成特定类型的模型。在这个简单任务里，我们将使用 DNNClassifier 预制估算器。\n",
    "* 估算器（基类）- 允许您使用 model_fn 函数完全掌控模型的创建方式。我们将在单独的博文中介绍如何操作。\n",
    "\n",
    "下面是估算器的类图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://1.bp.blogspot.com/-njTtnjOq_cE/Wbe772URrgI/AAAAAAAAD1Y/h1mWj6MGSzYg_KDuVXWBYeNqA4z5WRSpACLcBGAs/s1600/image2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如您所看到的，所有估算器都使用 **input_fn**，它为估算器提供输入数据。在我们的示例中，我们将重用 **my_input_fn**，这个函数是我们专门为演示定义的。\n",
    "\n",
    "下面的代码可以将预测鸢尾花类型的估算器实例化："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 构建特征列，这个我们会在后面的notebook里再细说\n",
    "feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]\n",
    "\n",
    "# 用DNNClassifier去构建一个MLP预估器\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "   # 预估所需的特征\n",
    "   feature_columns=feature_columns,\n",
    "   # 两个隐层，都是10个神经元\n",
    "   hidden_units=[10, 10],\n",
    "   # 都是3个类别\n",
    "   n_classes=3,\n",
    "   # 存储checkpoints的地方\n",
    "   model_dir=PATH)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在有了一个可以开始训练的估算器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 训练模型\n",
    "使用一行 TensorFlow 代码执行训练：\n",
    "```python\n",
    "# 使用之前写的my_input_fn产出输入到模型的数据，8个epoch\n",
    "classifier.train(\n",
    "   input_fn=lambda: my_input_fn(FILE_TRAIN, True, 8))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个“lambda: my_input_fn(FILE_TRAIN, True, 8)”将数据集与估算器连接起来。\n",
    "\n",
    "估算器需要数据来执行训练、评估和预测，它使用 input_fn 提取数据。估算器需要一个没有参数的 input_fn，因此我们将使用 lambda 创建一个没有参数的函数，这个函数会使用所需的参数 file_path, shuffle setting, 和 repeat_count 调用 input_fn。\n",
    "\n",
    "在这个例子中，我们使用 my_input_fn,，并向其传递：\n",
    "* FILE_TRAIN，训练数据文件。\n",
    "* True，告知估算器打乱数据。\n",
    "* 8，告知估算器将数据集重复 8 次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 评估我们经过训练的模型\n",
    "好了，我们现在有了一个经过训练的模型。如何评估它的性能呢？每个估算器都包含一个 evaluate 函数可以完成这个功能："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 和训练的数据输入形式一样\n",
    "evaluate_result = estimator.evaluate(\n",
    "   input_fn=lambda: my_input_fn(FILE_TEST, False, 4)\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "   print(\"   {}, was: {}\".format(key, evaluate_result[key]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们的示例中，我们达到了 93% 左右的准确率。当然，可以通过多种方式提高准确率。\n",
    "* 一种方式是多迭代几轮。由于模型的状态将持久保存（在上面的 model_dir=PATH 中），对它训练的迭代越多，模型在训练集上学习得会更充分一些。\n",
    "* 另一种方式是调整隐藏层的数量或每个隐藏层中节点的数量。你可以随意调整；不过请注意，在进行更改时，您需要移除在 model_dir=PATH 中指定的目录，因为您更改的是 DNNClassifier 的结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 使用我们经过训练的模型进行预测\n",
    "我们现在已经有一个经过训练的模型了，如果我们觉得评估结果还可以，可以使用这个模型根据一些输入来预测鸢尾花。与训练和评估一样，我们使用一个函数调用进行预测："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 用predict函数，注意这里只预估一次\n",
    "predict_results = classifier.predict(\n",
    "    input_fn=lambda: my_input_fn(FILE_TEST, False, 1))\n",
    "print(\"Predictions on test file\")\n",
    "for prediction in predict_results:\n",
    "   # 会给出预估的类别\n",
    "   print prediction[\"class_ids\"][0] \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 基于内存中的数据进行预测\n",
    "之前展示的代码将 FILE_TEST 指定为基于文件中存储的数据进行预测，不过，如何根据其他来源（例如内存）中的数据进行预测呢？正如您可能猜到的一样，进行这种预测不需要对我们的 predict 调用进行更改。不过，我们需要将 Dataset API 配置为使用如下所示的内存结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 构建内存中的数据，这里是3个样本\n",
    "prediction_input = [[5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n",
    "                    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris Virginica\n",
    "                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Sentosa\n",
    "def new_input_fn():\n",
    "   def decode(x):\n",
    "       # 切分4列\n",
    "       x = tf.split(x, 4)\n",
    "       # 因为是预测阶段，是不需要label的\n",
    "       # 构建字典返回\n",
    "       return dict(zip(feature_names, x))\n",
    "\n",
    "   # from_tensor_slices函数使用内存数据\n",
    "   dataset = tf.data.Dataset.from_tensor_slices(prediction_input)\n",
    "   # 同样decode为字典格式驶入\n",
    "   dataset = dataset.map(decode)\n",
    "   # 迭代器\n",
    "   iterator = dataset.make_one_shot_iterator()\n",
    "   next_feature_batch = iterator.get_next()\n",
    "   # 注意最后返回的并没有label\n",
    "   return next_feature_batch, None\n",
    "\n",
    "# 预测类别\n",
    "predict_results = classifier.predict(input_fn=new_input_fn)\n",
    "\n",
    "# 输出预测结果\n",
    "print(\"Predictions on memory data\")\n",
    "for idx, prediction in enumerate(predict_results):\n",
    "   type = prediction[\"class_ids\"][0] # Get the predicted class (index)\n",
    "   if type == 0:\n",
    "       print(\"I think: {}, is Iris Sentosa\".format(prediction_input[idx]))\n",
    "   elif type == 1:\n",
    "       print(\"I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n",
    "   else:\n",
    "       print(\"I think: {}, is Iris Virginica\".format(prediction_input[idx])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.过程监控与可视化\n",
    "使用像 DNNClassifier 一样的估算器可以提供很多值。除了易于使用外，预制估算器还提供内置的评估指标，并创建您可以在 TensorBoard 中看到的汇总。要查看此报告，请按照下面所示从您的命令行启动 TensorBoard：\n",
    "```python\n",
    "# 用训练时候的model_dir替换这里的PATH\n",
    "tensorboard --logdir=PATH \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面几个图显示了 TensorBoard 将提供的一些数据："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://4.bp.blogspot.com/-8fagWLn5Otc/Wbe8Fap8ltI/AAAAAAAAD1c/eSU7q5MDMH4zOansdXA3o6gVtr5LfEnFQCLcBGAs/s1600/image1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
