{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯与应用\n",
    "by 寒小阳(hanxiaoyang.ml@gmail.com)\n",
    "\n",
    "\n",
    "## 贝叶斯理论简单回顾\n",
    "\n",
    "在我们有一大堆样本（包含特征和类别）的时候，我们非常容易通过统计得到  p(特征|类别) .\n",
    "\n",
    "大家又都很熟悉下述公式：\n",
    "\n",
    "$$ p(x)p(y \\vert x)=p(y)p(x \\vert y)$$\n",
    " \n",
    "所以做一个小小的变换\n",
    "\n",
    "$$p(特征)p(类别 \\vert 特征)=p(类别)p(特征\\vert 类别)$$\n",
    " \n",
    "$$ p(类别 \\vert 特征)=p(类别)p(特征 \\vert 类别)p(特征) $$ \n",
    " \n",
    "## 独立假设\n",
    "\n",
    "看起来很简单，但实际上，你的特征可能是很多维的\n",
    "\n",
    "$$p(\\mathrm{features} \\vert \\mathrm{class})=p(f_0,f_1, \\cdots ,f_n \\vert c)$$\n",
    " \n",
    "就算是2个维度吧，可以简单写成\n",
    "\n",
    "$$p(f_0,f_1 \\vert c)=p(f_1 \\vert c,f_0)p(f_0\\vert c)$$\n",
    " \n",
    "这时候我们加一个特别牛逼的假设：特征之间是独立的。这样就得到了\n",
    "\n",
    "$$ p(f_0,f_1 \\vert c)=p(f_1\\vert c)p(f_0 \\vert c)$$ \n",
    " \n",
    "其实也就是：\n",
    "\n",
    "$$ p(f_0,f_1, \\cdots ,f_n \\vert c)=\\prod^n_ip(f_i \\vert c)$$ \n",
    " \n",
    "## 贝叶斯分类器\n",
    "OK，回到机器学习，其实我们就是对每个类别计算一个概率 p(c_i) ，然后再计算所有特征的条件概率 p(f_j|c_i) ，那么分类的时候我们就是依据贝叶斯找一个最可能的类别：\n",
    "\n",
    "$$p(\\mathrm{class}_i|f_0,f_1,\\cdots,f_n)= \\frac{p(\\mathrm{class}_i)}{p(f_0,f_1,\\cdots,f_n)}\\prod^n_jp(f_j|c_i)$$\n",
    "\n",
    "## 文本分类问题\n",
    "下面我们来看一个文本分类问题，经典的新闻主题分类，用朴素贝叶斯怎么做。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import jieba  #处理中文\n",
    "#import nltk  #处理英文\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#粗暴的词去重\n",
    "def make_word_set(words_file):\n",
    "    words_set = set()\n",
    "    with open(words_file, 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            word = line.strip()\n",
    "            if len(word)>0 and word not in words_set: # 去重\n",
    "                words_set.add(word)\n",
    "    return words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本处理，也就是样本生成过程\n",
    "def text_processing(folder_path, test_size=0.2):\n",
    "    folder_list = os.listdir(folder_path)\n",
    "    data_list = []\n",
    "    class_list = []\n",
    "\n",
    "    # 遍历文件夹\n",
    "    for folder in folder_list:\n",
    "        new_folder_path = os.path.join(folder_path, folder)\n",
    "        files = os.listdir(new_folder_path)\n",
    "        # 读取文件\n",
    "        j = 1\n",
    "        for file in files:\n",
    "            if j > 100: # 怕内存爆掉，只取100个样本文件，你可以注释掉取完\n",
    "                break\n",
    "            with open(os.path.join(new_folder_path, file), 'r') as fp:\n",
    "                raw_text = fp.read()\n",
    "            ## 是的，随处可见的jieba中文分词\n",
    "            jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数，不支持windows\n",
    "            word_cut = jieba.cut(raw_text, cut_all=False) # 精确模式，返回的结构是一个可迭代的genertor\n",
    "            word_list = list(word_cut) # genertor转化为list，每个词unicode格式\n",
    "            jieba.disable_parallel() # 关闭并行分词模式\n",
    "            \n",
    "            data_list.append(word_list) #训练集list\n",
    "            class_list.append(folder) #类别\n",
    "            j += 1\n",
    "    \n",
    "    ## 粗暴地划分训练集和测试集\n",
    "    data_class_list = list(zip(data_list, class_list))\n",
    "    random.shuffle(data_class_list)\n",
    "    index = int(len(data_class_list)*test_size)+1\n",
    "    train_list = data_class_list[index:]\n",
    "    test_list = data_class_list[:index]\n",
    "    train_data_list, train_class_list = zip(*train_list)\n",
    "    test_data_list, test_class_list = zip(*test_list)\n",
    "    \n",
    "    #其实可以用sklearn自带的部分做\n",
    "    #train_data_list, test_data_list, train_class_list, test_class_list = sklearn.cross_validation.train_test_split(data_list, class_list, test_size=test_size)\n",
    "    \n",
    "\n",
    "    # 统计词频放入all_words_dict\n",
    "    all_words_dict = {}\n",
    "    for word_list in train_data_list:\n",
    "        for word in word_list:\n",
    "            if word in all_words_dict :\n",
    "                all_words_dict[word] += 1\n",
    "            else:\n",
    "                all_words_dict[word] = 1\n",
    "\n",
    "    # key函数利用词频进行降序排序\n",
    "    all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f:f[1], reverse=True) # 内建函数sorted参数需为list\n",
    "    all_words_list = [w for w, c in all_words_tuple_list] # list(zip(*all_words_tuple_list)[0])\n",
    "\n",
    "    return all_words_list, train_data_list, test_data_list, train_class_list, test_class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_dict(all_words_list, deleteN, stopwords_set=set()):\n",
    "    # 选取特征词\n",
    "    feature_words = []\n",
    "    n = 1\n",
    "    for t in range(deleteN, len(all_words_list), 1):\n",
    "        if n > 1000: # feature_words的维度1000\n",
    "            break\n",
    "            \n",
    "        if not all_words_list[t].isdigit() and all_words_list[t] not in stopwords_set and 1<len(all_words_list[t])<5:\n",
    "            feature_words.append(all_words_list[t])\n",
    "            n += 1\n",
    "    return feature_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本特征\n",
    "def text_features(train_data_list, test_data_list, feature_words, flag='nltk'):\n",
    "    def text_features(text, feature_words):\n",
    "        text_words = set(text)\n",
    "        ## -----------------------------------------------------------------------------------\n",
    "        if flag == 'nltk':\n",
    "            ## nltk特征 dict\n",
    "            features = {word:1 if word in text_words else 0 for word in feature_words}\n",
    "        elif flag == 'sklearn':\n",
    "            ## sklearn特征 list\n",
    "            features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        else:\n",
    "            features = []\n",
    "        ## -----------------------------------------------------------------------------------\n",
    "        return features\n",
    "    train_feature_list = [text_features(text, feature_words) for text in train_data_list]\n",
    "    test_feature_list = [text_features(text, feature_words) for text in test_data_list]\n",
    "    return train_feature_list, test_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类，同时输出准确率等\n",
    "def text_classifier(train_feature_list, test_feature_list, train_class_list, test_class_list, flag='nltk'):\n",
    "    ## -----------------------------------------------------------------------------------\n",
    "    if flag == 'nltk':\n",
    "        ## 使用nltk分类器\n",
    "        train_flist = zip(train_feature_list, train_class_list)\n",
    "        test_flist = zip(test_feature_list, test_class_list)\n",
    "        classifier = nltk.classify.NaiveBayesClassifier.train(train_flist)\n",
    "        test_accuracy = nltk.classify.accuracy(classifier, test_flist)\n",
    "    elif flag == 'sklearn':\n",
    "        ## sklearn分类器\n",
    "        classifier = MultinomialNB().fit(train_feature_list, train_class_list)\n",
    "        test_accuracy = classifier.score(test_feature_list, test_class_list)\n",
    "    else:\n",
    "        test_accuracy = []\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn' has no attribute 'cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m## 文本预处理\u001b[39;00m\n\u001b[1;32m      4\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/usr/local/codeData/jupyterData/jupyter-bd/bayesClassifier/SogouC/Sample\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m all_words_list, train_data_list, test_data_list, train_class_list, test_class_list \u001b[38;5;241m=\u001b[39m \u001b[43mtext_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 生成stopwords_set\u001b[39;00m\n\u001b[1;32m      8\u001b[0m stopwords_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/usr/local/codeData/jupyterData/jupyter-bd/bayesClassifier/stopwords_cn.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m, in \u001b[0;36mtext_processing\u001b[0;34m(folder_path, test_size)\u001b[0m\n\u001b[1;32m     35\u001b[0m test_data_list, test_class_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtest_list)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#其实可以用sklearn自带的部分做\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m train_data_list, test_data_list, train_class_list, test_class_list \u001b[38;5;241m=\u001b[39m \u001b[43msklearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_validation\u001b[49m\u001b[38;5;241m.\u001b[39mtrain_test_split(data_list, class_list, test_size\u001b[38;5;241m=\u001b[39mtest_size)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 统计词频放入all_words_dict\u001b[39;00m\n\u001b[1;32m     42\u001b[0m all_words_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sklearn' has no attribute 'cross_validation'"
     ]
    }
   ],
   "source": [
    "print(\"start\")\n",
    "\n",
    "## 文本预处理\n",
    "folder_path = \"/usr/local/codeData/jupyterData/jupyter-bd/bayesClassifier/SogouC/Sample\"\n",
    "all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = text_processing(folder_path, test_size=0.2)\n",
    "\n",
    "# 生成stopwords_set\n",
    "stopwords_file = '/usr/local/codeData/jupyterData/jupyter-bd/bayesClassifier/stopwords_cn.txt'\n",
    "stopwords_set = make_word_set(stopwords_file)\n",
    "\n",
    "## 文本特征提取和分类\n",
    "# flag = 'nltk'\n",
    "flag = 'sklearn'\n",
    "deleteNs = range(0, 1000, 20)\n",
    "test_accuracy_list = []\n",
    "for deleteN in deleteNs:\n",
    "    # feature_words = words_dict(all_words_list, deleteN)\n",
    "    feature_words = words_dict(all_words_list, deleteN, stopwords_set)\n",
    "    train_feature_list, test_feature_list = text_features(train_data_list, test_data_list, feature_words, flag)\n",
    "    test_accuracy = text_classifier(train_feature_list, test_feature_list, train_class_list, test_class_list, flag)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "print(test_accuracy_list)\n",
    "\n",
    "# 结果评价\n",
    "#plt.figure()\n",
    "plt.plot(deleteNs, test_accuracy_list)\n",
    "plt.title('Relationship of deleteNs and test_accuracy')\n",
    "plt.xlabel('deleteNs')\n",
    "plt.ylabel('test_accuracy')\n",
    "plt.show()\n",
    "#plt.savefig('result.png')\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
